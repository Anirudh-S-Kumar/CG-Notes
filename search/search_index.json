{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Computer Graphics","text":"<p>Notes I make while taking the course of Computer Graphics. The course is taught by Prof. Ojaswa Sharma. It's advised to use these as a reference/revision instead of learning from them(although you can do that). Some images that were present in lecture slides were not able to be added, and notes regarding referring to the lecture slides for illustrations is added </p>"},{"location":"#my-personal-review-of-the-course","title":"My personal review of the course","text":"<p>If you are even mildly interested in this topic, then this course will scratch that itch that you have. The course is very well structured, and the professor is fantastic at explaining the concepts and addressing doubts. </p>"},{"location":"advanced_ray_tracing/","title":"Advanced Ray Tracing","text":""},{"location":"advanced_ray_tracing/#antialiasing","title":"Antialiasing","text":"<p>Antialiasing is a technique used to reduce the appearance of jagged edges in images. It is particularly useful in ray tracing, where the jagged edges are caused by the discrete nature of the pixels in the image. Simple way to antialias an image is to average color in a small neighborhood around each pixel. </p>"},{"location":"advanced_ray_tracing/#regular-sampling","title":"Regular Sampling","text":"<p>Sub-sample a pixel in regular grid fashion by shooting out rays passing through sub-pixels sampled regularly</p> <p>Replace  <pre><code>for each pixel (i, j) in the image:\n    c_ij = ray_color(i + 0.5, j + 0.5)\n</code></pre> By <pre><code>for each pixel (i, j) in the image:\n    c_ij = 0\n    for (p,q) in (0, n-1) x (0, n-1):\n        c_ij += ray_color(i + (p + 0.5) / n, j + (q + 0.5) / n)\n\n    c_ij /= n^2\n</code></pre></p> <p>Where <code>n</code> is the number of sub-pixels in the pixel.</p> Regular Sampling <p>Drawback</p> <p>Regular sampling can cause Moire pattern in the image. Moire patterns happen when two sets of lines or grids overlap and make a new, wavy or swirly pattern.</p> <p> Moire Pattern"},{"location":"advanced_ray_tracing/#random-sampling","title":"Random Sampling","text":"<p>Choosing sub-pixels randomly instead of regular grid. Select n^2 random points in the pixel and average the color of the rays passing through them. </p> <pre><code>for each pixel (i, j) in the image:\n    c_ij = 0\n    for k in 1 to n^2:\n        p = random()\n        q = random()\n        c_ij += ray_color(i + p, j + q)\n\n    c_ij /= n^2\n</code></pre> <p>Where <code>random()</code> returns a random number between 0 and 1.</p> Random Sampling <p>Drawback</p> <p>Random sampling can cause noise in the image. Noise is the random variation of brightness or color in an image.</p>"},{"location":"advanced_ray_tracing/#jittered-sampling","title":"Jittered Sampling","text":"<p>Jittered sampling is a compromise between regular and random sampling. It is a form of random sampling where the random points are chosen in a regular grid but with some random offset. </p> <pre><code>for each pixel (i, j) in the image:\n    c_ij = 0\n    for (p,q) in (0, n-1) x (0, n-1):\n        p += random()\n        q += random()\n        c_ij += ray_color(i + (p + 0.5) / n, j + (q + 0.5) / n)\n\n    c_ij /= n^2\n</code></pre> Jittered Sampling <p>Poisson Disk Sampling</p> <p>Poisson Disk Sampling is a more advanced sampling technique that ensures that no two samples are too close to each other by ensuring distance between all samples is at least \\(r\\). This helps in reducing noise in the image while maintaining the randomness of the samples.</p>"},{"location":"advanced_ray_tracing/#transformations-in-ray-tracing","title":"Transformations in Ray Tracing","text":"<p>Either transform object to world coordinates or transform ray to object coordinates.</p> <p>Create a wrapper class <code>TransformedSurface</code> that takes a surface and a transformation matrix and transforms the ray to object coordinates before intersecting with the surface.</p> <pre><code>class TransformedSurface : public Surface\n\nprivate:\n    Surface* surface;\n    Matrix4x4 transform;\n\npublic:\n    bool intersect(const Ray&amp; ray);\n</code></pre> <p>To intersect the ray with the transformed surface, transform the ray to object coordinates, intersect with the surface, and then transform the hit point back to world coordinates.</p> <pre><code>bool TransformedSurface::intersect(const Ray&amp; ray) {\n    Ray transformed_ray = ray.transform(transform.inverse());\n    transformed_hit = surface-&gt;intersect(transformed_ray);\n    return transformed_hit.transform(transform);\n}\n</code></pre>"},{"location":"advanced_ray_tracing/#groups-and-hierarchies","title":"Groups and Hierarchies","text":"<p>Often it's useful to group surfaces together to apply transformations to all of them at once. This can be done by creating a <code>SurfaceGroup</code> class that contains a list of surfaces and a transformation matrix, which returns the closest surface hit by the ray.</p> <pre><code>class SurfaceGroup : public Surface\n\nprivate:\n    std::vector&lt;Surface*&gt; surfaces;\n    Matrix4x4 transform;\n\npublic:\n    bool intersect(const Ray&amp; ray);\n</code></pre> <p>A common optimization is to merge transforms with Groups</p> Transformations in Ray Tracing"},{"location":"advanced_ray_tracing/#instancing","title":"Instancing","text":"<ul> <li>Transform objects in several ways</li> <li>One surface could have multiple <code>TransformedSurface</code> instances with different transformations</li> <li>Allowing this makes the transformation tree a DAG (Directed Acyclic Graph)</li> </ul> Instancing"},{"location":"advanced_ray_tracing/#some-more-considerations","title":"Some more considerations","text":"<p>Transforming rays is expensive</p> <ul> <li>minimize tree depth<ul> <li>push all transformations towards the leaves</li> <li>merge transformations</li> </ul> </li> <li>internal group nodes still required for instancing<ul> <li>can't push two transforms down to same child</li> </ul> </li> </ul>"},{"location":"advanced_ray_tracing/#ray-tracing-acceleration","title":"Ray Tracing Acceleration","text":"<p>Most of the time is spent in ray-surface intersection tests Was to reduce reduce this time :-</p> <ul> <li>Make intersection tests faster, but this is hard and can only go so far</li> <li>Reduce the number of intersection tests<ul> <li>Intersection with every object is not necessary</li> <li>Basic strategy: efficiently find big chunks of geometry that can be skipped</li> </ul> </li> </ul>"},{"location":"advanced_ray_tracing/#bounding-volumes-bvol","title":"Bounding Volumes (bvol)","text":"<p>Bounding volumes are simple shapes that enclose a more complex shape. They are used to quickly determine if a ray intersects a complex shape by first checking if it intersects the bounding volume.</p> <ul> <li>If the ray doesn't intersect the bounding volume, it can't intersect the complex shape</li> <li>If the ray intersects the bounding volume, then check the complex shape</li> </ul> <p>Some more considerations</p> <ul> <li>Cost of bvol intersection test should be low. Therefore, use simple shapes like spheres, boxes, etc.</li> <li>Cost of object intersection test should be high. Therefore, bvol is most useful when the object is complex.</li> <li>Tightness of bvol is important. A tight bvol will reduce the number of intersection tests.</li> </ul> Bounding Volume"},{"location":"advanced_ray_tracing/#choice-of-bounding-volume","title":"Choice of Bounding Volume","text":"<ul> <li>Sphere: Easy to intersect, but not tight</li> <li>Axis-aligned bounding box (AABB): Easy to intersect, tight, especially for axis-aligned objects</li> <li>Oriented bounding box (OBB): Easy to intersect, but cost of transformation, tighter for arbitrary objects</li> </ul> <p>Computing the bvol</p> <ul> <li>For simple shapes like spheres and AABBs, it's easy to compute the bounding volume</li> <li>For groups: not so easy for OBBs</li> <li>For transformed objects: not so easy for spheres</li> </ul>"},{"location":"advanced_ray_tracing/#implementation","title":"Implementation","text":"<p>Using a new Surface subclass <code>BoundedSurface</code> that contains a bounding volume and a surface. The <code>BoundedSurface</code> class first checks if the ray intersects the bounding volume and then checks if the ray intersects the surface.</p> <pre><code>class BoundedSurface : public Surface\n\nprivate:\n    Surface* surface;\n    BoundingVolume* bvol;\n\npublic:\n    bool intersect(const Ray&amp; ray);\n</code></pre> <pre><code>bool BoundedSurface::intersect(const Ray&amp; ray) {\n    if (!bvol-&gt;intersect(ray)) {\n        return false;\n    }\n\n    return surface-&gt;intersect(ray);\n}\n</code></pre>"},{"location":"advanced_ray_tracing/#bounding-volume-hierarchies-bvh","title":"Bounding Volume Hierarchies (BVH)","text":"<p>Using a hierarchy of bounding volumes to reduce the number of intersection tests. The bounding volume hierarchy is a tree where each node contains a bounding volume that encloses all the objects in its children.</p> Bounding Volume Hierarchy"},{"location":"advanced_ray_tracing/#building-the-bvh","title":"Building the BVH","text":"<p>Done in a top-down fashion, making a bbox that encloses all objects and then splitting it into two smaller boxes. This is done recursively until each box contains a small number of objects.</p> <p>How to partition the objects?</p> <ul> <li>Ideal: Clusters</li> <li>Practical: Splitting the bounding box along the longest axis<ul> <li>Center Partition - Efficient, but can be unbalanced</li> <li>Median Partition - Balanced, but more expensive</li> <li>SAH (Surface Area Heuristic) - Model expected cost of ray intersection, produces best performance</li> </ul> </li> </ul>"},{"location":"advanced_ray_tracing/#space-subdivision","title":"Space Subdivision","text":"<p>Instead of partitioning objects, we partition space instead. We can do partitioning in a regular fashion (grid) or in a non-regular fashion (octree, kd-tree).</p> <p>Regular Grid division simply divides space into a grid and assigns objects to the grid cells. This is simple and easy to implement, but can be inefficient if objects are not uniformly distributed.</p>"},{"location":"advanced_ray_tracing/#non-regular-space-subdivision","title":"Non-Regular Space Subdivision","text":"<ul> <li>kd-tree: subdivides space, like a grid, but it is adaptive like BVH</li> </ul> kd-tree"},{"location":"animations/","title":"Animation","text":"<p>Animation is specifying shape as a function a time, but we don't want to specify the shape of the object at every point in time. Instead, we want to specify the shape of the object at a few keyframes, and let the computer interpolate the shape of the object between the keyframes. This is called keyframe animation, i.e. draw important poses first, and let the computer fill in the rest.</p> <p>Thus, we can divide the animation process into two steps:</p> <ul> <li>Creating high-level controls for adjusting geometry</li> <li>Interpolate these controls over time between keyframes</li> </ul>"},{"location":"animations/#affine-transformations","title":"Affine Transformations","text":"<p>Recap</p> <p>Affine transformations are rotations, translations, and scaling. They are linear transformations that preserve points, straight lines, and planes. They are represented by a 4x4 matrix and have the general form \\(f = \\mathbf{A}x + \\mathbf{b}\\), where \\(\\mathbf{A}\\) is an invertible map, and \\(\\mathbf{b}\\) is some translation</p> <p>Transformations in animation can be of two types:</p> <ul> <li>Global or Affine Transformations</li> <li>Local, which are hierarchy of affine, or point-wise affine transformations</li> </ul> <p>If we naively interpolate between two affine transformations, we might not get the desired result. Consider the following example:</p> <p>We have a square that we want to turn by 45 degrees, something like the following:</p>  Linearly Interpolating Transformations <p>Where let \\(\\mathbf{M}_0 = I\\) and \\(\\mathbf{M}_1 = R\\), where \\(R\\) is rotation by angle \\(\\theta\\) Then the linearly interpolated transformation \\(\\mathbf{M}_t\\) is:</p> \\[ \\begin{align*} \\mathbf{M}_t &amp;= I + t(R - I) \\\\ &amp;= \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix} + t \\left( \\begin{bmatrix} \\cos \\theta &amp; -\\sin \\theta \\\\ \\sin \\theta &amp; \\cos \\theta \\end{bmatrix} - \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix} \\right) \\\\ &amp;= \\begin{bmatrix} 1 + t(\\cos \\theta - 1) &amp; -t \\sin \\theta \\\\ t \\sin \\theta &amp; 1 + t(\\cos \\theta - 1) \\end{bmatrix} \\end{align*} \\] <p>This should be a valid rotation matrix, but it is not. Any valid rotation matrix must have the following 3 properties</p> <ul> <li>Rows must be orthogonal</li> <li>Columns must be orthogonal</li> <li>Determinant must be +1</li> </ul> <p>Let's verify each of these properties for \\(\\mathbf{M}_t\\):</p> <ul> <li>Row orthogonality: \\((1 + t(\\cos \\theta - 1))(t \\sin \\theta) + (-t \\sin \\theta) \\cdot (1 + t(\\cos \\theta - 1))  = 0\\)</li> <li>Column orthogonality: \\((1 + t(\\cos \\theta - 1))(t \\sin \\theta) + (-t \\sin \\theta) \\cdot (1 + t(\\cos \\theta - 1))  = 0\\)</li> <li>Determinant:</li> </ul> \\[ \\begin{align*} \\text{det}(\\mathbf{M}_t) &amp;= (1 + t(\\cos \\theta - 1))^2 + t^2 \\sin^2 \\theta \\\\ &amp;= 1 + 2t(\\cos \\theta - 1) + t^2(\\cos^2 \\theta - 2 \\cos \\theta + 1) + t^2 \\sin^2 \\theta \\\\ &amp;= 1 + 2t(\\cos \\theta - 1) + t^2(\\cos^2 \\theta + \\sin^2 \\theta - 2 \\cos \\theta + 1) \\\\ &amp;= 1 + 2t(\\cos \\theta - 1) + t^2(2 - 2 \\cos \\theta) \\\\ &amp;= 1 + 2t(\\cos \\theta - 1) + 2t^2(1 - \\cos \\theta) \\\\ \\end{align*} \\] <p>For \\(\\text{det}(\\mathbf{M}_t) = 1\\), we need \\(2t(\\cos \\theta - 1) + 2t^2(1 - \\cos \\theta) = 0\\) </p> \\[2t(\\cos \\theta - 1) + 2t^2(1 - \\cos \\theta) = (1 - \\cos \\theta) 2t(t - 1) = 0\\] <p>i.e. \\(t = 0\\) or \\(t = 1\\). This means that the linearly interpolated transformation is not a valid rotation matrix for any \\(t \\neq 0, 1\\).</p>"},{"location":"animations/#then-how-to-correctly-interpolate-transformations","title":"Then how to correctly interpolate transformations?","text":"<p>\\(\\mathbf{M}\\) is a function of \\(\\theta, t, s\\) for rotation, translation, and scaling respectively. We can interpolate these parameters linearly and then construct the transformation matrix. </p> <p>Rotations in 3D belong to a group called \\(SO(3)\\), which stands for Special Orthogonal group in 3D. Special as \\(\\det{R} = 1\\) and Orthogonal as \\(R^T R = I\\). </p>"},{"location":"animations/#spherical-linear-interpolation-slerp","title":"Spherical Linear Interpolation (SLERP)","text":"<p>Say we want to interpolate from vector \\(\\mathbf{v}_0\\) to \\(\\mathbf{v}_1\\). We can't just interpolate the vectors directly, as the interpolated vector might not be a unit vector. The vector connecting the vector is </p> \\[w = \\hat{v_1} + (\\hat{v_0} \\cdot \\hat{v_1}) \\hat{v_0}\\] <p>Angle between the vectors is \\(\\theta = \\cos^{-1}(\\hat{v_0} \\cdot \\hat{v_1})\\). We can interpolate the angle and then construct the vector.</p> \\[\\hat{v(t)} = (\\cos t\\theta) \\hat{v_0} + (\\sin t\\theta) \\hat{w} \\quad \\text{where } t \\in [0, 1]\\]  Spherical Linear Interpolation <p>There are various ways of parameterizing rotations, but each of them have their own pros and cons. Below we discuss 3 ways of parameterizing rotations:</p> <ul> <li>Euler Angles</li> <li>Axis-Angle</li> <li>Quaternions</li> </ul>"},{"location":"animations/#euler-angles","title":"Euler Angles","text":"<ul> <li>Rotate around x-axis , then y-axis, then z-axis. Do note that these axis are in the object's local coordinate system, and not in global coordinate system.</li> <li>Memory efficient and simple to understand</li> </ul> \\[f(\\alpha, \\beta, \\gamma) = R_z(\\gamma) R_y(\\beta) R_x(\\alpha)\\] <p>The problem with Euler angles is gimble lock, where two axes align and the third axis is lost.</p>"},{"location":"animations/#axis-angle","title":"Axis-Angle","text":"<ul> <li>Specify axis to rotate around, and angle by which to rotate</li> <li>Multiply axis and angle to get a more compact form </li> </ul> \\[f(\\mathbf{a}, \\theta) = R_{\\hat{a}}(||\\mathbf{a}||)\\] <p>Problem with Axis-angle is with separate rotation angle, there are mutliple ways to represent the same rotation.</p> <p>Even with combined rotation angle, making small changes near 180\\(^\\circ\\) can lead to large changes in the rotation.</p>"},{"location":"animations/#quaternions","title":"Quaternions","text":"<p>A quaternion is an extension of complex numbers</p> \\[q = (s, v) = (s, v_1, v_2, v_3)\\] <p>We partition \\(\\sqrt{-1}\\) as \\(i, j, k\\) and then define multiplication as:</p> \\[i^2 = j^2 = k^2 = ijk = -1\\] <p>They have a cyclic relationship with multiplication i.e.</p> \\[ij = k, jk = i, ki = j \\text{ and } ji = -k, kj = -i, ik = -j\\] <p>Hence, we define the set of quaternion as:</p> \\[\\mathbb{H} = \\{a + bi + cj + dk | (a, b, c, d) \\in \\mathbb{R}^4\\}\\]"},{"location":"animations/#properties-of-quaternions","title":"Properties of Quaternions","text":"Property Description Conjugate \\(\\bar{q} = (s, -v) = a - bi - cj - dk\\) Norm \\(\\|q\\| = \\sqrt{q \\bar{q}} = \\sqrt{a^2 + b^2 + c^2 + d^2}\\) Associative \\(q_1(q_2q_3) = q_1q_2q_3 = (q_1q_2)q_3\\) Not Commutative \\(q_1q_2 \\neq q_2q_1\\) Multiplication of Magnitudes \\(\\|q_1q_2\\| = \\|q_1\\| \\|q_2\\|\\) <p>For unit quaternions, \\(\\|q\\| = 1\\), \\(\\bar{q} = q^{-1}\\)</p>"},{"location":"animations/#unit-quaternions","title":"Unit Quaternions","text":"<p>The set of unit-magnitude quaternions is denoted as \\(S^3\\). They are used to represent rotations in 3D space.</p> <ul> <li>\\(|q_1| = |q_2| = 1 \\implies |q_1q_2| = 1\\)</li> <li>\\(q_1, q_2 \\in S^3 \\implies q_1q_2 \\in S^3\\)</li> </ul>"},{"location":"animations/#scalar-vector-representation","title":"Scalar Vector Representation","text":"\\[q = (s, v) = (s, v_1, v_2, v_3) = s + v_1i + v_2j + v_3k\\] <p>Multiplication of two quaternions is then:</p> \\[(s_1, v_1)(s_2, v_2) = (s_1s_2 - v_1 \\cdot v_2, s_1v_2 + s_2v_1 + v_1 \\times v_2)\\] <p>For unit quaternions, \\(|s|^2 + |v|^2 = 1\\), so we can think of these as sine ans cosine of and angle \\(\\psi\\), so \\(q = (\\cos \\psi, \\mathbf{\\hat{v}} \\sin \\psi )\\) or \\(q = \\cos \\psi + \\mathbf{\\hat{v}} \\sin \\psi\\) (similar to how we write complex numbers)</p>"},{"location":"animations/#quaternions-and-rotations","title":"Quaternions and Rotations","text":"<p>There is a natural association between the unit quaternion and the 3D axis-angle rotation. If \\(cos \\psi + \\mathbf{\\hat{v}} \\sin \\psi \\in S^3\\), and \\(R_{\\hat{v}}(\\theta)\\) is the rotation matrix, then: \\(\\theta = 2\\psi\\)</p> <p>Represent a point in space by a purely-imaginary quaternion</p> \\[\\mathbf{x} = (x, y, z) \\in \\mathbb{R}^3 \\leftrightarrow X = xi + yj+ zk \\in \\mathbb{H}\\] <p>Then, we can compute rotations by quaternion \\(q\\) as:</p> \\[X_{\\text{rot}} = qX\\bar{q}\\] <p>And we can do multiple rotations by as well as </p> \\[q_1(q_2X\\bar{q_2})\\bar{q_1} = (q_1q_2)X\\bar{q_2}\\bar{q_1} = (q_1q_2)X\\overline{q_1q_2}\\] <p>If we write a unit quaternion as \\(q = \\cos \\psi + \\mathbf{\\hat{v}} \\sin \\psi\\), then operation:</p> \\[X_{\\text{rot}} = qX\\bar{q} = (\\cos \\psi + \\mathbf{\\hat{v}} \\sin \\psi) X (\\cos \\psi - \\mathbf{\\hat{v}} \\sin \\psi)\\] <p>This is equivalent to rotating the point by angle \\(2\\psi\\) around axis \\(\\mathbf{\\hat{v}}\\), so an alternate explanation is \"A quaternion is just a slightly different way to encode an axis and angle in 4 numbers, than than a number \\(\\theta\\) and a vector \\(\\mathbf{v}\\), we store the number \\(\\cos \\frac{\\theta}{2}\\) and the vector \\(\\sin \\frac{\\theta}{2} \\mathbf{v}\\)\"</p> <p>But why quaternions?</p> <ul> <li>Fast, fewer operations</li> <li>Numerically stable for incremental changes</li> <li>Composes rotations nicely</li> <li>Convert to matrices at the end</li> <li>Biggest reason is spherical linear interpolation (SLERP)</li> </ul>"},{"location":"animations/#slerp-using-quaternions","title":"SLERP using Quaternions","text":"Spherical Linear Interpolation using Quaternions <p>From here, \\(\\alpha + \\beta = \\psi\\), and \\(v(t) = w_0 v_0 + w_1 v_1\\)</p> \\[\\frac{\\sin \\alpha}{w_1} = \\frac{\\sin \\beta}{w_0} = \\frac{\\sin (\\pi - \\psi)}{1} = \\sin \\psi\\] <p>From here, we get \\(w_0 = \\frac{\\sin \\beta}{\\sin \\psi}\\) and \\(w_1 = \\frac{\\sin \\alpha}{\\sin \\psi}\\) and \\(\\psi = \\cos^{-1}(v_0 \\cdot v_1)\\)</p> <p>Therefore, finally we can write the SLERP as:</p> \\[ \\begin{align*} v(t) &amp;= \\frac{\\sin \\beta}{\\sin \\psi} v_0 + \\frac{\\sin \\alpha}{\\sin \\psi} v_1 \\\\ &amp;= \\frac{\\sin(\\psi - \\alpha) v_0 + \\sin(\\alpha) v_1}{\\sin \\psi} \\\\ &amp;= \\frac{\\sin((1-t)\\psi) v_0 + \\sin(t\\psi) v_1}{\\sin \\psi} &amp; \\text{where } \\alpha = t \\psi, t \\in [0, 1] \\end{align*} \\] <p>When angle gets close to zero, estimation of \\(\\psi\\) is inaccurate. For small angles, use linear interpolation instead. If \\(q_0 q_1 &gt; 0\\), then slerp between then, else slerp between \\(q_0\\) and \\(-q_1\\)</p>"},{"location":"animations/#animation-handles","title":"Animation Handles","text":"<p>We design objects using a hierarchy of handles which have predefined degrees of freedom. Animation handles are high-level controls that allow us to adjust the geometry of the object. They are used to specify the shape of the object at keyframes.</p> <p>The main handle types are </p> <ul> <li>Point Handle</li> <li>Line handles or skeleton handles</li> <li>Cage handles</li> </ul>"},{"location":"animations/#kinematics-and-inverse-kinematics","title":"Kinematics and Inverse Kinematics","text":"<p>Relationship between DOFs and 3D pose is called kinematics. A kinematic chain is a sequence of rigid bodies connected by joints. There are mainly 3 parts when talking about kinematics</p> <ul> <li>Root: The first body in the chain</li> <li>End Effector: The last body in the chain</li> <li>Joints: The connections between the bodies</li> </ul> <p>Forward Kinematics deals with finding the pose of the end effector given the joint angles. Inverse Kinematics deals with finding the joint angles given the pose of the end effector.</p>"},{"location":"animations/#surface-deformation-techniques","title":"Surface Deformation Techniques","text":""},{"location":"animations/#mesh-skinning","title":"Mesh Skinning","text":"<p>Mesh skinning is a simple way to deform a surface to follow a skeleton. Let's assume surface as control points \\(p_i\\), and each bone has a transformation matrix \\(M_j\\) (normally a rigid motion), and every point-bone pair has a weight \\(w_{ij}\\).</p> <p>Then the deformed point \\(p_i'\\) is a weighted sum of the transformations of the bones:</p> \\[p_i' = \\sum_{j} w_{ij} M_j p_i\\] <p>This method is also called linear blend skinning. This is a simple and fast method, but it has some issues: </p> <ul> <li>Surface collapses on the inside of bends and in presence of strong twists. A fix for this is using dual-quaternion skinning instead.</li> </ul>"},{"location":"animations/#blend-shapes","title":"Blend Shapes","text":"<p>We just provide a few key poses and interpolate between them. User provides all the key shapes, i.e. a position for every control point in every shape \\(p_{ij}\\), for point \\(i\\) in shape \\(j\\), and also a weight \\(w_j\\) for each shape. It's essential that \\(\\sum_j w_j = 1\\) for every frame</p> <p>Then the deformed point \\(p_i'\\) is:</p> \\[p_i' = \\sum_j w_j p_{ij}\\] <p>This works well for relatively small motions, like faces.</p> <p>Note</p> <p>Some topics are not covered here like Motion Capture. Interested readers can find more about them online, in the slides or in the book.</p>"},{"location":"colors/","title":"Colors","text":""},{"location":"colors/#measuring-light","title":"Measuring Light","text":"<ul> <li>Colorimetry is the science of measuring light and color.</li> <li>Sensations that arise from light energy of different wavelengths</li> <li>Color is a phenomenon of human perception and not a universal property of light</li> <li>The photodetectors in human retina consists of rods and cones<ul> <li>Rods : Sensitive to brightness</li> <li>Cones : Sensitive to color </li> </ul> </li> </ul>"},{"location":"colors/#cone-responses","title":"Cone Responses","text":"<ul> <li>Three types of cones in human retina<ul> <li>S-cones : Sensitive to short wavelengths</li> <li>M-cones : Sensitive to medium wavelengths</li> <li>L-cones : Sensitive to long wavelengths</li> </ul> </li> <li>Response of a cone is magnitude of electrical signal generated in response to light of a particular wavelength. \\(\\phi(\\lambda)\\) is the wavelength density function of the incident light, and \\(L(\\lambda)\\), \\(M(\\lambda)\\), and \\(S(\\lambda)\\) are the response functions of the L, M, and S cones, respectively. The response of the cones can be expressed as:</li> </ul> \\[ L = \\int_{\\lambda} \\phi(\\lambda) L(\\lambda) d\\lambda \\] \\[ M = \\int_{\\lambda} \\phi(\\lambda) M(\\lambda) d\\lambda \\] \\[ S = \\int_{\\lambda} \\phi(\\lambda) S(\\lambda) d\\lambda \\]"},{"location":"colors/#colorometric-concepts","title":"Colorometric Concepts","text":"<ul> <li>Luminance : Brightness of a color</li> <li>Chromaticity : Color without brightness</li> <li>Dominant Wavelegth : Single spectral color (hue)</li> <li>Purity : Ratio of pure color to white light (saturation)</li> </ul>"},{"location":"colors/#color-matching","title":"Color matching","text":"<p>Photoreceptors act as linear intergrators of light energy. This means it's possible to find two different spectral distributions \\(-\\phi_1(\\lambda)\\) and  \\(-\\phi_2(\\lambda)\\) that produce the same response in the cones. This is the basis of color matching. This is called metamerism. </p> <ul> <li>Spectral tri-stimulus values - using monochromatic light sources to match a given color</li> <li>CIE defined three primaries: 435.8 nm(B), 546.1 nm(G) and 700.0 nm(R)</li> <li>A color can be matched by a linear combination of the three primaries</li> <li>Negative values means that wavelength is too saturated to be produced by the primary</li> </ul> <p>It's possible to transform one set of tristimulus values to another</p> <ul> <li>CIE defined a standard observer with color matching functions \\(X(\\lambda)\\), \\(Y(\\lambda)\\), and \\(Z(\\lambda)\\)</li> <li>Cannot be realized physically</li> </ul>"},{"location":"colors/#color-spaces","title":"Color spaces","text":"<ul> <li>We could define an orthogonal coordinate system with \\(X\\), \\(Y\\), and \\(Z\\) as the axes</li> <li>Color gamut: spacial extent of volume in which colors lie</li> </ul> \\[ x = \\frac{X}{X+Y+Z} \\hspace{1cm} y = \\frac{Y}{X+Y+Z} \\hspace{1cm} z = \\frac{Z}{X+Y+Z} \\] \\[ X = \\frac{x}{y} Y \\hspace{1cm} Z = \\frac{1-x-y}{y} Y \\] <p>We usually represent Z in terms of X and Y, and Y is the luminance.</p>"},{"location":"colors/#chromaticity-diagram","title":"Chromaticity Diagram","text":"Chromaticity Diagram <ul> <li>All colors visible to the human eye lie within the horseshoe shaped curve</li> <li>The straight line connecting two points on the curve represents all the colors that can be produced by mixing the two colors</li> <li>The edge of the diagram, called the specral locus, represents pure mono-chromatic colors. These are the most saturated colors</li> <li>The least saturated colors are at the center of the diagram, where the white point is located</li> <li>Color gamut: subet of colors that can be produced by a particular device</li> </ul>"},{"location":"colors/#other-color-spaces","title":"Other color spaces","text":"Space Description Cons RGB Additive color model, Based on human perception of color Not perceptually uniform, High correlation between components HSV Hue, Saturation, Value Hue discontinuity at 0 and 360 degrees, Bad correlation between computed and perceived lightness CMYK Subtractive color model, used in printing"},{"location":"curves/","title":"Curves","text":"<p>Path of a continuously moving point in the space. Set of all points where the moving point has been.</p>"},{"location":"curves/#mathematical-description","title":"Mathematical Description","text":"<ul> <li>Parametric Curves:<ul> <li>In 2D: \\(C(t) = (x(t), y(t)), C: \\mathbb{R} \\rightarrow \\mathbb{R^2}\\)</li> <li>In 3D: \\(C(t) = (x(t), y(t), z(t)), C: \\mathbb{R} \\rightarrow \\mathbb{R^3}\\)</li> </ul> </li> <li>Implicit Curves: Defined as the set of points satisfying an equation \\(f(x, y) = 0\\).<ul> <li>In 2D: \\(C = \\{(x, y): f(x, y) = 0 \\}\\)</li> <li>In 3D: \\(C = \\{(x, y, z): f(x, y, z) = 0 \\}\\)</li> </ul> </li> </ul> <p>Warning</p> <p>Always ensure that Implicit curves are of the form \\(f(x, y) = 0\\) and not \\(f(x, y) = c\\).</p>"},{"location":"curves/#some-representations-of-popular-curves","title":"Some representations of popular curves","text":"Curve Parametric Implicit Line \\(p(t) = p_0 + (p_1 - p_0)t, t \\in (-\\infty, \\infty)\\) \\(C = \\{(x, y): ax + by + c = 0\\}\\) Circle \\(p(t) = (r\\cos(t), r\\sin(t)), t \\in [0, 2\\pi]\\) \\(C = \\{(x, y): x^2 + y^2 - r^2 = 0\\}\\) <p>Note</p> <p>Always mention the range of the parameter \\(t\\) when defining a parametric curve.</p> <p>Parametric Polynomial Curves: \\(C(t) = \\{x(t), y(t)\\}\\) is polynomial iff \\(x(t)\\) and \\(y(t)\\) are polynomials in \\(t\\).</p>"},{"location":"curves/#approximation-curves","title":"Approximation Curves","text":"<p>Do not need to interpolate all the points(i.e. pass through all the points). Approximation curves are used to represent the shape of the curve.</p> <p>Examples:</p> <ul> <li>Bezier Curves</li> <li>B-Spline Curves</li> <li>Catmull-Rom Splines</li> <li>etc.</li> </ul>"},{"location":"curves/#bezier-curves","title":"Bezier Curves","text":"Quadratic Bezier Curve <p>Bezier curves are defined by a set of control points. Nth order Bezier curve is defined by N+1 control points. Equation of Linear Bezier curve is given by:</p> \\[B^1(t) = (1-t)P_0 + tP_1\\] <p>where \\(P_0\\) and \\(P_1\\) are the control points. Bezier curves are defined recursively. The general equation for Nth order Bezier curve is given by:</p> \\[B^n(t) = (1-t)B^{n-1}(t) + tB^{n-1}(t)\\] <p>or they can be defined using all the control points as:</p> \\[B^n(t) = \\sum_{i=0}^{n} {n \\choose i} (1-t)^{n-1}t^i P_i\\] <p>where \\({n \\choose i}\\) is the binomial coefficient, and \\(P_i\\) are the control points.</p>"},{"location":"curves/#properties-of-bezier-curves","title":"Properties of Bezier Curves","text":"<ul> <li>The curve always passes through the end points, i.e. \\(B^n(0) = P_0\\) and \\(B^n(1) = P_n\\).</li> <li>The curve is a straight line iff all the control points are collinear.</li> <li> <p>The start and end tangent vectors are parallel to the line joining the first and last control points, i.e.</p> \\[\\frac{dB}{dt}|_{t = 0} \\propto \\overrightarrow{P_0 P_1} \\hspace{40px} \\text{ and } \\hspace{40px} \\frac{dB}{dt}|_{t = 1} \\propto \\overrightarrow{P_{n-1} P_n}\\] </li> <li> <p>Convex Hull Property: The curve lies within the convex hull of the control points.</p> </li> </ul> Convex Hull (the boundary is P0 P1 P2 P3) <p>Note</p> <p>Convex Hull is the smallest convex set that contains all the control points. Convex set is a set where the line segment joining any two points in the set lies entirely within the set.</p> <ul> <li>Each \\(n\\)th order Bezier curve has an equally shaped \\(n+1\\)th order Bezier curve</li> <li>Is Affine Invariant: The curve remains the same under affine transformations (i.e. translation, rotation, scaling, and shear).</li> </ul> Rational Bezier Curves <p>Rational Bezier curves are defined by dividing each control point by a weight. The equation for Rational Bezier curve is given by:</p> \\[B^n(t) = \\frac{\\sum_{i=0}^{n} {n \\choose i} (1-t)^{n-1}t^i w_i P_i}{\\sum_{i=0}^{n} {n \\choose i} (1-t)^{n-1}t^i w_i}\\] <p>where \\(w_i\\) are the weights. They have better local control and can represent conic sections, but they are computationally expensive.</p>"},{"location":"curves/#interpolation-curves","title":"Interpolation Curves","text":"<p>Given a set of points \\(P_0, P_1, \\dots, P_n\\) , we want to construct a curve that passes through all the points. \\(C(k) = p_k\\) where \\(k = 0, 1, \\dots, n-1\\).</p> <p>Some interpolation curves are:</p> <ul> <li>Lagrange Interpolation</li> <li>Piecewise Bezier Curves</li> <li>Piecewise B-Spline Curves</li> <li>etc.</li> </ul> <p> </p> Different types of interpolation"},{"location":"curves/#lagrange-interpolation","title":"Lagrange Interpolation","text":"<p>Given a set of points \\(P_0, P_1, \\dots, P_n\\), the Lagrange interpolation polynomial is given by:</p> \\[L(k) = \\sum_{i=0}^{n} P_i l_i(k)\\] <p>where \\(l_i(k)\\) are the Lagrange basis functions given by:</p> \\[l_i(k) = \\prod_{j=0, j \\neq i}^{n} \\frac{k - k_j}{k_i - k_j}\\] <p>We rarely use Lagrange interpolation in practice because of the huge oscillations and large interpolation error.</p>"},{"location":"curves/#piecewise-interpolation-curves","title":"Piecewise Interpolation Curves","text":"<p>Known as \"Poly-Curves\". Each segment  between two interpolation points is represented by a curve. </p> <p> </p> Two types of interpolation, Linear on the right and Cubic Bezier on the left"},{"location":"curves/#continuity-in-piecewise-curves","title":"Continuity in Piecewise Curves","text":"<p>Parametric Continuity \\(C^n\\)</p> <ul> <li>Segments have equal \\(n\\)th derivative at the junction points.</li> <li>Tangents have equal direction and magnitude at the junction points.</li> </ul> Continuity Description \\(C^-1\\) Curve is discontinuous \\(C^0\\) Curve is continuous \\(C^1\\) First Derivative is continuous \\(C^2\\) Second Derivative is continuous \\(C^n\\) \\(n\\)th Derivative is continuous <p>Geometric Continuity \\(G^n\\): Tangents have equal direction but not magnitude at the junction points. Curve is \\(G^n\\) continuous if it can be reparametrized to be \\(C^n\\) continuous.</p> Continuity Description \\(G^0\\) Curve touch at the joint point (=\\(C^0\\)) \\(G^1\\) Curves share a common tangent direction \\(G^2\\) Curves share a common center of curvature"},{"location":"implicit_modelling/","title":"Implicit Modelling and Reconstruction","text":"<p>An implicit function is defined as a function \\(f\\) applied to a point \\(p\\) yielding a scalar value \\(\\in \\mathbb{R}\\), For example the implicit representation of a circle is \\(C(r) = \\{(x, y): x^2 + y^2 - r^2 = 0 \\}\\)</p>"},{"location":"implicit_modelling/#implicit-modelling","title":"Implicit Modelling","text":""},{"location":"implicit_modelling/#distance-fields","title":"Distance Fields","text":"<p>Distance field is defined with respect to some geometric object \\(T\\)</p> \\[\\mathbf{F}(T, \\mathbf{p}) = \\min_{\\mathbf{q} \\in T} \\|\\mathbf{p} - \\mathbf{q}\\|\\] <p>\\(\\mathbf{F}(T, \\mathbf{p})\\) is shortest distance from point \\(\\mathbf{p}\\) to the object \\(T\\), where \\(T\\) is any geometric entity embedded in 3D space. A property that all distance fields share is the following</p> \\[|\\nabla F| = 1\\] <p>This means the gradient of the distance field is always a unit vector. A way to think about this is that if you move along the gradient of the distance field, you will move exactly one unit of distance.</p>"},{"location":"implicit_modelling/#signed-distance-fields","title":"Signed Distance Fields","text":"<p>Same idea as distance fields, but attach a sign based on inside/outside w.r.t. the surface. In a discrete setting, usually computed over a set of pixels (e.g. on an image) or in a volumetric grid.</p> Signed Distance Field <p>A discrete computation of a SDF is \\(\\mathcal{O}(n^2)\\) in 2D and \\(\\mathcal{O}(n^3)\\) in 3D, where \\(n\\) is the size of the grid.</p>"},{"location":"implicit_modelling/#level-set-of-an-implicit-function","title":"Level-set of an implicit function","text":"<p>The level-set of an implicit function \\(f\\) is the set of points where \\(f\\) evaluates to a constant value. For example, the level-set of a circle is the circle itself.</p> Level-set. Blue is the level set, Red is the gradient"},{"location":"implicit_modelling/#implicit-function-representation","title":"Implicit Function Representation","text":"<p>An implicit function \\(f_i(x, y, z)\\) maybe be split into two parts</p> <ul> <li>A distance function \\(d_i(x, y, z)\\)</li> <li>A fall-off filter function \\(g_i(r)\\), where \\(r\\) is the distance from skeleton</li> </ul> \\[f_i(x, y, z) = g_i \\circ d_i(x, y, z)\\]"},{"location":"implicit_modelling/#fall-off-filter-functions","title":"Fall-off Filter functions","text":"<p>Field values are maximum at the skeleton, which fall off to zero at a certain distance from the skeleton(finite support). In case of simple blending, global field is the sum of all field values at a point.</p> \\[f(x, y, z) = \\sum_{i=1}^{n} f_i(x, y, z)\\] Filter name Mathematical form Blinn/Blobby \\(g(d) = e^{-rd^2}\\) Metaballs \\(g(d) = \\begin{cases} 1 - 3 (\\frac{d}{r})^2 &amp; 0 \\leq d \\leq \\frac{2}{3} \\\\ \\frac{3}{2}(1 - \\frac{d}{r})^2 &amp; \\frac{r}{3} \\leq d \\leq r \\\\ 0 &amp; d &gt; r \\end{cases}\\) Soft Objects \\(g(d) = \\begin{cases} 1 - \\frac{4 d^6}{9 r ^6} + \\frac{17 d^4}{9 r^4} - \\frac{22 d^2}{9 r^2} &amp; 0 \\leq d \\leq r \\\\ 0 &amp; d &gt; r \\end{cases}\\) Wyvill \\(g(d) = \\begin{cases} \\left(1 - \\frac{d^2}{r^2}\\right)^3 &amp; 0 \\leq d \\leq r \\\\ 0 &amp; d &gt; r \\end{cases}\\) <p>Except Blinn, all of them have a finite support. </p> <p>Note</p> <p>See notes for good illustrations</p>"},{"location":"implicit_modelling/#skeletal-primitives","title":"Skeletal Primitives","text":"<p>In order to define an implicit function field, distance to the skeletal primitives need to be computed. This is simple for point primitives, but tricky for complex geometries.</p>"},{"location":"implicit_modelling/#line-segment-skeleton","title":"Line Segment skeleton","text":"<p>A line segment primitive (AB) can be defined as the cylinder around a line with hemispherical end caps. </p> Line Segment <p>use \\(AC\\) to compute the distance \\(CP_0\\)</p> \\[\\vec{AC} = \\vec{AB} \\frac{AP_0 \\cdot AB}{||AB||^2}\\] <p>Then use \\(CP_0^2 + AC^2 = AP_0^2\\)</p> <p>Some basic skeletal primitives of 3D objects are</p> Object Skeletal Primitive Sphere Point Cylinder Line Segment Cone Line Segment Torus Circle Cylinder Line Segment Disc Circle Cube Cube"},{"location":"implicit_modelling/#convolution-surfaces","title":"Convolution surfaces","text":"<p>Surfaces produced by convolving a geometric skeleton \\(S\\) with a kernel function \\(h\\)</p> \\[f(p) = \\int_S g(r) h(p - r) dr \\] <p>where \\(h\\) has finite support.</p>"},{"location":"implicit_modelling/#super-elliptic-blending","title":"Super-elliptic blending","text":"<p>Given two functions \\(f_A\\) and \\(f_B\\), denote a more general blending operator as \\(A \\diamond B\\) The Ricchi blend is defined as </p> \\[f_{A \\diamond B} = \\left( f_A^{n} + f_B^n \\right)^\\frac{1}{n}\\] <p>where \\(n\\) is a parameter that controls the blending. For \\(n=1\\), this is the same summation, and for \\(n=\\infty\\), this is the same as union of the two fields.</p> \\[ \\begin{align*} \\lim_{n \\to \\infty} \\left(f_A^n + f_B^n \\right)^\\frac{1}{n} &amp;= \\max(f_A, f_B) \\\\ \\lim_{n \\to -\\infty} \\left(f_A^n + f_B^n \\right)^\\frac{1}{n} &amp;= \\min(f_A, f_B) \\\\ \\end{align*} \\]"},{"location":"implicit_modelling/#rendering","title":"Rendering","text":"<p>Rendering an implicit function requires searching for a surface at an iso level. There are two main techniques used to render implicit surfaces </p>"},{"location":"implicit_modelling/#ray-tracing","title":"Ray Tracing","text":"<p>Continue marching along the ray until the sign of the distance field changes. This is the point where the surface is. Then find the normal at that point and shade it.</p> <p>We have two points \\(p\\) and \\(q\\) and the implicit surface \\(\\phi\\) such that \\(\\phi(p) &gt; 0\\) and \\(\\phi(q) &lt; 0\\). The surface is somewhere between \\(p\\) and \\(q\\). How to find the root?</p> Root finding <ul> <li>Solve for algebraic expression if \\(\\phi\\) is known, but this is not always possible</li> <li>Linear interpolation between \\(p\\) and \\(q\\) to find the root. This is not always accurate</li> <li>Iterative root finding/Newton's method/Binary search</li> </ul> <p>Note</p> <p>Only works if the step size is small enough to capture the surface.</p>"},{"location":"implicit_modelling/#polygonization","title":"Polygonization","text":"<p>Algorithms based on numerical continuation</p> <ul> <li>Divide the space into cubic voxels. Implicit field is sampled at the voxel centres in a volumetric grid(consider a dual grid)</li> <li>Search for surface starting from a skeletal element(a starting point)</li> <li>Add voxel to queue, mark it visited</li> <li>Search neighbors</li> <li>When done, replace voxel with polygons</li> </ul>"},{"location":"implicit_modelling/#phase-1-marching-cubes","title":"Phase 1: Marching Cubes","text":"<ul> <li>Starting from the skeletal elements, search for a voxel containing at least one intersecting edge</li> <li>From this seed voxel, traverse neighbors to find the other voxels containing intersecting edges</li> <li>Add such voxels to a queue for processing </li> </ul> Marching Cubes <p>To find if edge passes through the voxel, check the sign of the field at all 4 corners of the voxel. If all signs are not the same, then the edge passes through the voxel. Caveat is voxel size should be small enough to capture the surface.</p> Edge Test"},{"location":"implicit_modelling/#phase-2-polygonization","title":"Phase 2: Polygonization","text":"<p>Each voxel is replaced by a set of triangles that best match the shape of the surface in that voxel. </p> <p>There are 256 (\\(2^8\\)) possible cases, but only 14 unique cases.</p>"},{"location":"implicit_modelling/#problem","title":"Problem","text":"<ul> <li>Ambiguous cases: When opposite corners of a voxel have same sign and other corners have the other sign</li> <li>Under-sampling: When the voxel size is too large to capture the surface</li> </ul>"},{"location":"implicit_modelling/#constructive-solid-geometry","title":"Constructive Solid Geometry","text":"<p>Constructive Solid Geometry (CSG) is a technique to create complex shapes by combining simple shapes using boolean operations. The basic operations are union, intersection, and difference. The object is stored as a tree, where the leaves are the simple shapes(primitives) and the intermediate nodes are the boolean operations.</p> CSG Operation Mathematical form Union \\(\\bigcup_\\max f = \\max_{i = 0}^{k-1} (f_i)\\) Intersection \\(\\bigcap_\\max f = \\min_{i = 0}^{k-1} (f_i)\\) Difference \\(\\min\\max f = \\min \\left( f_0, 2 * iso - \\max_{j = 1}^{k-1} (f_j) \\right)\\) <p>CSG Operators create creases, i.e. \\(C^1\\) discontinuities.</p>"},{"location":"implicit_modelling/#warping","title":"Warping","text":"<p>Distorting the shape of a surface by warping the space in it's neighborhood. A warp is a continuous function \\(w(x, y, z): \\mathbb{R}^3 \\to \\mathbb{R}^3\\). Define the warped by applying \\(w\\) to the implicit equation</p> \\[f_i(p) = d_i \\circ g_i \\circ w_i(p)\\] <p>Where \\(d_i \\circ g_i\\) is the original implicit function.</p> <p>Some warps are </p> Warp Mathematical form Twist \\(w(x, y, z) = \\begin{Bmatrix} x \\cos(\\theta(z)) - y \\sin(\\theta(z)) \\\\ x \\sin(\\theta(z)) + y \\cos(\\theta(z)) \\\\ z \\end{Bmatrix}\\) represents twist around z-axis Taper \\(s(y) = \\frac{y_\\max - y}{y_\\max - y_\\min}\\), \\(w(x, y, z) = \\begin{Bmatrix} s(y)x \\\\ y \\\\ s(y)z \\end{Bmatrix}\\) for taper along y-axis Bend \\(w(x, y, z) = \\begin{Bmatrix} -\\sin(\\theta)(y - 1/k) + x_0 \\\\ \\cos(\\theta)(y - 1/k) + 1/k \\\\ z \\end{Bmatrix}\\) for bending around y-axis, \\(k\\) is the bending rate <p>Note</p> <p>See slides for good illustrations</p>"},{"location":"implicit_modelling/#surface-reconstruction","title":"Surface Reconstruction","text":"<p>Given a set of points, reconstruct the surface that passes through these points. This is a common problem in computer graphics, computer vision, and medical imaging.</p> <p>There are various methods to do it such as </p> <ul> <li>RBF Reconstruction</li> <li>Poisson Reconstruction</li> <li>MLS(Moving Least Squares) Reconstruction</li> </ul> <p>We will discuss only RBF Reconstruction here.</p>"},{"location":"implicit_modelling/#gaussian-rbf","title":"Gaussian RBF","text":"<p>Given values \\(f_i\\) at points \\(p_i\\), find a function </p> \\[s(p) = \\sum_{i} \\lambda_i \\phi(\\|p - p_i\\|)\\] <p>such that \\(s(p_i) = f(p_i)\\), where \\(\\phi(r) = e^{-\\alpha r^2}\\) is the Gaussian RBF. The weights \\(\\lambda_i\\) are found by solving a linear system of equations.</p> \\[f_i = \\sum_{j} \\lambda_j \\phi(\\|p_i - p_j\\|)\\] <p>In general, we need a polynomial term</p> \\[f_i = \\sum_{j} \\lambda_j \\phi(\\|p_i - p_j\\|) + P(p_i)\\] <p>where \\(P(p) = c + c_x p_x + c_y p_y + c_z p_z\\) is a polynomial term. Leads to a linear systems as following</p> \\[ \\begin{bmatrix} \\Phi &amp; P \\\\ P^T &amp; 0 \\end{bmatrix} \\begin{bmatrix} \\lambda \\\\ c \\end{bmatrix} = \\begin{bmatrix} f \\\\ 0 \\end{bmatrix} \\] <p>where \\(\\Phi_{ij} = \\phi(\\|p_i - p_j\\|)\\) is a \\(n \\times n\\) matrix, \\(P\\) is a \\(n \\times 4\\) matrix, and \\(f\\) is a \\(n \\times 1\\) vector.</p>"},{"location":"implicit_modelling/#regularization","title":"Regularization","text":"<p>Regularization is used to make the system les ill-conditioned. The regularized system is</p> \\[f_i = \\sum_{j} \\lambda_j \\phi(\\|p_i - p_j\\|) + P(p_i) + k \\lambda_i\\] <p>where \\(k\\) is the regularization constant, which leads to the following system</p> \\[ \\begin{bmatrix} \\Phi + kI &amp; P \\\\ P^T &amp; 0 \\end{bmatrix} \\begin{bmatrix} \\lambda \\\\ c \\end{bmatrix} = \\begin{bmatrix} f \\\\ 0 \\end{bmatrix} \\]"},{"location":"lighting/","title":"Lighting and Shading","text":"<ul> <li>A surface can emit light (self-emission) or reflect light from other sources.</li> <li>Recursive scattering of light between surfaces accounts for subtle shading effects. This can be expressed in the rendering equation.</li> <li>The rendering equation can either be physically based or empirical.</li> <li>Physically based rendering is very time consuming and expensive to compute, but it produces realistic images.</li> </ul> Lighting"},{"location":"lighting/#surface-types-based-on-light-matter-interaction","title":"Surface types based on light-matter interaction","text":"<ul> <li>Specular surfaces: Shiny surfaces that reflect light in a narrow range of directions close to the angle of reflection.</li> <li>Glossy surfaces: Surfaces that are shiny but not as shiny as specular surfaces.</li> <li>Diffuse surfaces: Rough surfaces that reflect light in all directions.</li> <li>Translucent surfaces: Surfaces that allow light to pass through them.</li> </ul> Surface Types"},{"location":"lighting/#physically-based-rendering-aka-global-illumination","title":"Physically Based Rendering a.k.a Global Illumination","text":""},{"location":"lighting/#some-terms","title":"Some Terms","text":"<ul> <li>Flux (\\(\\Phi(A)\\)): The amount of light energy passing through a surface per unit time. Unit is watts.</li> <li>Irradiance (\\(E(A)\\)): Flux per unit area arriving at a surface. Unit is watts per square meter.</li> </ul> \\[E(A) = \\frac{d\\Phi(A)}{dA(x)}\\] <ul> <li>Radioisity (\\(B(A)\\)): The total amount of light energy leaving a surface per unit time. Unit is watts.</li> </ul> <ul> <li>Radiance (\\(L(\\mathbf{x}, \\vec{\\omega})\\)): Flux density per unit solid angle, per perpendicular unit area</li> </ul> \\[L(\\mathbf{x}, \\vec{\\omega}) = \\frac{d^2\\Phi(A)}{d\\vec{\\omega}dA^\\perp (x, \\vec{\\omega})}\\] <ul> <li>Solid Angle (\\(\\Omega\\)): The angle subtended by a surface at a point in space. Unit is steradian.</li> </ul> \\[\\Omega = \\frac{A}{r^2}\\]"},{"location":"lighting/#bidirectional-reflectance-distribution-function-brdf","title":"Bidirectional Reflectance Distribution Function (BRDF)","text":"<ul> <li>Ratio between outgoing radiance for angle \\(\\omega_o\\) and incoming irradiance for angle \\(\\omega_i\\).</li> <li>BRDF describes Density w.r.t \\(\\omega_i\\). Integrating it over all incoming irradiance gives the total outgoing radiance.</li> </ul> \\[\\rho(\\omega_i, \\omega_o) = \\frac{d}{d \\omega_i} \\frac{L_o(\\omega_o)}{L_i(\\omega_i) \\cos{\\theta_i}}\\] BRDF BRDF for different surfaces"},{"location":"lighting/#properties-of-brdf","title":"Properties of BRDF","text":"<ul> <li>Positivity: \\(\\rho(\\omega_i, \\omega_o) \\geq 0\\)</li> <li>Reciprocity: \\(\\rho(\\omega_i, \\omega_o) = \\rho(\\omega_o, \\omega_i)\\)</li> <li>Energy Conservation: </li> </ul> \\[\\forall \\omega_i: \\int_{\\Omega} \\rho(\\omega_i, \\omega_o) d\\omega_o \\leq 1\\]"},{"location":"lighting/#the-rendering-equation","title":"The rendering equation","text":"\\[L_o(\\mathbf{x}, \\vec{\\omega_o}) = E_o(\\mathbf{x}, \\vec{\\omega_o}) + \\int_{\\Omega} \\rho(\\mathbf{x}, \\vec{\\omega_i}, \\vec{\\omega_o}) L_i(\\mathbf{x}, \\vec{\\omega_i}) \\cos{\\theta_i} d\\omega_i\\] <p>This is too complicated to understand, so we will reduce it to a simpler form.</p> <ul> <li>Unknown function \\(L\\) is related to linear operator (integration) on itself + emission function \\(E\\)</li> <li>Discrete<ul> <li>Functions \\(\\rightarrow\\) Vectors, Linear operators \\(\\rightarrow\\) Matrices</li> <li>\\(L = E + K \\cdot L\\)</li> <li>Linear system of equations</li> </ul> </li> <li>Rendering becomes solving a linear system of equations. </li> </ul> \\[ \\begin{align*} L &amp;= E + K \\cdot L \\\\ L &amp;= (I - K)^{-1} \\cdot E \\\\ &amp;= \\sum_{i=0}^{\\infty} K^i \\cdot E \\end{align*} \\] <p>The last line is formed by the Neumann series expansion of \\((I - K)^{-1}\\).</p>"},{"location":"lighting/#light-paths","title":"Light Paths","text":"<ul> <li>Heckbert's notation for light paths:</li> <li>Expresses light paths in terms of surface interactions</li> <li>\\(L\\) - Light source</li> <li>\\(E\\) - Eye</li> <li>\\(S\\) - Specular reflection/refraction</li> <li>\\(D\\) - Diffused reflection</li> </ul> <p>We can represent light paths as a regular expression</p> <ul> <li><code>*</code> - Zero or more occurrences</li> <li><code>+</code> - One or more occurrences</li> <li><code>?</code> - Zero or one occurrence</li> <li><code>|</code> - OR</li> </ul> <p>Heckbert's classification of light paths:</p> <ul> <li>Direct illumination: \\(L(D|S)E\\)</li> <li>Indirect illumination: \\(L(D|S)D(D|S)+E\\)</li> <li>Classical (Whitted) ray tracing: \\(LDS*E\\)</li> <li>Full global illumination: \\(L(D|S)*E\\)</li> </ul>"},{"location":"lighting/#approximate-models","title":"Approximate models","text":""},{"location":"lighting/#phong-reflection-model","title":"Phong Reflection Model","text":"<ul> <li>Blinn-Phong reflection model is a simplified model that approximates the shading of a surface.</li> <li>Efficient to compute and provides good results.</li> <li>Consists of three components: ambient, diffuse, and specular reflection.</li> </ul> \\[I = I_a + I_d + I_s\\] Phong Model"},{"location":"lighting/#ambient-reflection","title":"Ambient Reflection","text":"<ul> <li>Any point whose normal is not facing the light source will appear black.</li> <li>In reality, light bounces off other surfaces and illuminates the point.</li> <li>An ambient term is added to account for this.</li> </ul> \\[I_a = k_a \\cdot I_{a}, \\text{ where } 0 \\leq k_a \\leq 1 \\]"},{"location":"lighting/#diffuse-reflection","title":"Diffuse Reflection","text":"<ul> <li>Lambertian objects reflect light equally in all directions.</li> <li>No change in color with respect to the angle of view.</li> <li>Lambert's cosine law: intensity is proportional to the cosine of the angle between the light source and the normal. If \\(l\\) is the light vector (pointing towards the light source) and \\(n\\) is the normal vector, then the intensity \\(c\\) is given by:</li> </ul> \\[c \\propto n \\cdot l\\] <ul> <li>Reflection coefficient \\(k_d\\) represents fraction of light reflected.</li> </ul> \\[I_d = k_d L_d \\max((l \\cdot n), 0)\\]"},{"location":"lighting/#light-source-attenuation","title":"Light source Attenuation","text":"<ul> <li>Energy decreases as the inverse square with distance </li> <li>Therefore for diffuse lighting, </li> </ul> \\[I_d = k_d f_{att} L_d \\max((l \\cdot n), 0)\\] <p>where \\(f_{att}\\) is the light source attenuation function </p> <ul> <li>A common attenuation function is</li> </ul> \\[f_{att} = \\min\\left(\\frac{1}{a + bd + cd^2}, 1\\right)\\] <p>where \\(d\\) is the distance between the light source and the point, and \\(a\\), \\(b\\), and \\(c\\) are constants.</p>"},{"location":"lighting/#specular-reflection","title":"Specular Reflection","text":"<ul> <li>Phong lighting model adds a specular reflection term to the lighting equation.</li> <li>The amount of light that the viewer sees depends on the angle \\(\\phi\\) between \\(r\\) (reflected direction) and \\(v\\) (view vector).</li> </ul> \\[I_s = k_s L_s \\max((r \\cdot v)^n, 0)\\] <p>where \\(n\\) is the shininess coefficient.</p> <p>Reflected Vector</p> <p>Two fundamental conditions - Angle of incidence = Angle of reflection - At a point \\(p\\) on the surface, the incident light vector \\(l\\) and the reflected light vector \\(r\\) are coplanar with the normal vector \\(n\\) at \\(p\\).</p> <p> </p> <ul> <li>\\(\\theta_i = \\theta_r \\implies \\cos{\\theta_i} = \\cos{\\theta_r} \\implies l \\cdot n = r \\cdot n\\)</li> <li>\\(r = \\alpha n + \\beta l\\) </li> <li>\\(n \\cdot r = n \\cdot (\\alpha n + \\beta l) = \\alpha n \\cdot n + \\beta n \\cdot l = \\beta n \\cdot l = l \\cdot n\\)</li> <li>\\(l = r \\cdot r = \\alpha^2 + 2 \\alpha \\beta l \\cdot n + \\beta^2 = 1\\)</li> <li>\\(r = 2(n \\cdot l)n - l\\)</li> </ul>"},{"location":"lighting/#putting-it-all-together","title":"Putting it all together","text":"\\[I = \\frac{1}{a + bd + cd^2} (k_d L_d \\max((l \\cdot n), 0) + k_s L_s \\max((r \\cdot v)^n, 0)) + k_a I_a\\] <p>The diffuse and are computed and added for each light source.</p> <p>If there are multiple light sources, then separate diffuse and specular terms are computed for each light source and added together.</p> \\[I = k_a I_a + \\sum_{i=1}^{n} (f_{att} (k_{d_i} L_{d_i} \\max(l_i \\cdot n, 0)) + k_{s_i} L_{s_i} \\max((r_i \\cdot v)^n, 0))\\]"},{"location":"lighting/#modified-phong-model","title":"Modified Phong Model","text":"<ul> <li>With Phong model, the reflected vector \\(r\\) needs to be computed at every point on surface </li> <li>To avoid this expensive computation, halfway vector \\(h\\) is used.</li> </ul> \\[h = \\frac{l + v}{||l + v||}\\] <ul> <li>Replace \\(r \\cdot v\\) with \\(h \\cdot n\\).</li> <li>Phong highlights will be smaller than before</li> </ul> Modified Phong Model"},{"location":"lighting/#transparent-surfaces","title":"Transparent Surfaces","text":"<ul> <li>Both specular and diffuse transmissions can take place at the surfaces of a transparent surface. </li> <li>When a beam of light is incident on a transparent material, part of it is reflected and part of it is refracted.</li> </ul> <p>Snell's Law</p> <ul> <li>The angle of incidence and the angle of refraction are related by Snell'js Law.</li> <li>\\(\\eta_i \\sin{\\theta_i} = \\eta_r \\sin{\\theta_r}\\)</li> <li>\\(\\eta_i\\) and \\(\\eta_r\\) are the refractive indices of the incident and refracted media respectively.</li> <li>\\(\\theta_i\\) and \\(\\theta_r\\) are the angles of incidence and refraction respectively.  </li> </ul> Refraction <ul> <li>Refraction occurs whenever light passes from one medium to another with a different refractive index.</li> <li>Law of refraction: Incident ray \\(d\\), refracted ray \\(t\\) and normal \\(n\\) are coplanar.</li> </ul> <p>Let \\(n\\) and \\(b\\) be basis vectors of the plane containing \\(d\\) and \\(t\\).</p> <ul> <li>\\(t = \\sin{\\theta_r} b - \\cos{\\theta_r} n\\)</li> <li>\\(d = \\sin{\\theta_i} b - \\cos{\\theta_i} n \\implies b = \\frac{d + n \\cos{\\theta_i}}{\\sin{\\theta_i}}\\)</li> </ul> \\[ \\begin{align*} t &amp;= \\frac{\\eta_i}{\\eta_r} (d + n\\cos{\\theta_i}) - n \\cos{\\theta_r}\\\\ &amp;= \\frac{\\eta_i}{\\eta_r} (d - n(d \\cdot n)) - n \\sqrt{1 - \\left(\\frac{\\eta_i}{\\eta_r}\\right)^2 (1 - (d \\cdot n)^2)}\\\\ \\end{align*} \\]"},{"location":"lighting/#fresnel-equations","title":"Fresnel Equations","text":"<ul> <li>Reflectivity of a dielectric surface varies with the angle of incidence according to the Fresnel equations.</li> <li>Schlick's approximation is a common way to approximate the Fresnel equations.</li> </ul> \\[ R(\\theta) = R_0 + (1 - R_0)(1 - \\cos{\\theta})^5\\] <p>where \\(R_0\\) is the reflectance at normal incidence, and \\(\\theta\\) is the angle of incidence.</p> \\[R_0 = \\left(\\frac{\\eta_r - 1}{\\eta_r + 1}\\right)^2\\] <p>Thus,</p> \\[I = (1 - R(\\theta)) I_{trans} + R(\\theta) T_{refl}\\]"},{"location":"lighting/#polygon-rendering-methods","title":"Polygon Rendering Methods","text":""},{"location":"lighting/#flat-shading","title":"Flat Shading","text":"<ul> <li>All points within a face of the model are assigned the same color.</li> <li>All vertices of the face have same normal.</li> <li>Gives a faceted look to the model.</li> </ul>"},{"location":"lighting/#gouraud-shading","title":"Gouraud Shading","text":"<ul> <li>Also known as intensity interpolation.</li> <li>Linearly interpolates vertex intensities across the face.</li> <li>All vertices of the face have different normals.</li> </ul>"},{"location":"lighting/#phong-shading","title":"Phong Shading","text":"<ul> <li>Also known as normal vector interpolation.</li> <li>Interpolates normal vectors across the face instead of intensities.</li> <li>All points within a face have different normals.</li> </ul>"},{"location":"mesh/","title":"Mesh","text":""},{"location":"mesh/#introduction-to-mesh-and-its-properties","title":"Introduction to Mesh and it's properties","text":"<p>Piecewise linear approximation with error \\(O(h^2)\\) is called a mesh</p> <p>Mesh elements</p> <ul> <li>Face: Subset of a 3d plane</li> <li>Edge: Incident points of 2 or more faces</li> <li>Vertex: Incident points of 2 or more edges</li> </ul> <p>Mesh Local Structure</p> <ul> <li>Element type: Triangle, Quad meshes, or polygon meshes. We always use triangles as they are always planar.</li> <li>Element shape: Isotropic, i.e. locally uniform in all directions, or anisotropic, i.e. non-uniform in all directions.</li> <li>Element density: Uniform or non-uniform. Non-uniform density is used to refine the mesh in regions of interest.</li> </ul> <p>Note</p> <p>For better illustrations, see lecture slides on google classroom. I might add some illustrations here in the future myself but for now, I will just leave it as it is.</p> <p>Regularity of Mesh</p> <ul> <li>Irregular: any number of irregular vertices</li> <li>Semi-regular: small number of irregular vertices</li> <li>Highly regular: most vertices are regular</li> <li>Regular: all vertices are regular</li> </ul> <p>Info</p> <p>A Vertex is regular if it is incident to 6 edges. We generally use regular meshes as they are easier to work with.</p>"},{"location":"mesh/#mesh-data-structures","title":"Mesh Data Structures","text":""},{"location":"mesh/#face-set","title":"Face Set","text":"<p>It is simply a list of faces. Each face is represented by a list of vertices.</p> Faces \\(f_1 = (v_{11}, v_{12}, v_{13})\\) \\(f_2 = (v_{21}, v_{22}, v_{23})\\) \\(\\vdots\\) \\(f_n = (v_{n1}, v_{n2}, v_{n3})\\) Face Vertex \\(v_1 = (x_1, y_1, z_1)\\) Vertex \\(v_2 = (x_2, y_2, z_2)\\) Vertex \\(v_3 = (x_3, y_3, z_3)\\) Face Set"},{"location":"mesh/#indexed-face-set","title":"Indexed Face Set","text":"<p>This time we are using indices to represent the vertices. This is useful when we have a large number of vertices and faces. This reduces redundancy and makes the data structure more compact.</p> Face VertexRef \\(v_1, v_2, v_3\\) FaceRef \\(f_1, f_2, f_3\\) FaceData data Vertex Point \\((x, y, z)\\) FaceRef data VertexData data Vertices \\(v_1 =  (x_1, y_1, z_1)\\) \\(v_2 =  (x_2, y_2, z_2)\\) \\(\\vdots\\) \\(v_n =  (x_n, y_n, z_n)\\) Faces \\(f_1 = (i_{11}, i_{12}, i_{13})\\) \\(f_2 = (i_{21}, i_{22}, i_{23})\\) \\(\\vdots\\) \\(f_n = (i_{n1}, i_{n2}, i_{n3})\\) Indexed Face Set"},{"location":"mesh/#winged-edge","title":"Winged Edge","text":"<p>This is a more complex data structure. Each Vertex and Face have a reference to an edge along with some other data. Each edge has the following</p> <ul> <li>Vertex references (\\(v_0\\) being the source and \\(v_1\\) being the target)</li> <li>Face references (\\(f_L\\) and \\(f_R\\))</li> <li>Previous and Next edge references for the left and right face</li> <li>Edge data</li> </ul> <p>Warning</p> <p>Edges in a face always follow anti-clockwise order.</p> Vertex Point position EdgeRef edge VertexData data Face EdgeRef edge FaceData data Edge VertexRef \\(v_0\\) \\(v_1\\) FaceRef \\(f_L\\) \\(f_R\\) EdgeRef \\(e_{{prevL}}\\) \\(e_{{prevR}}\\) EdgeRef \\(e_{{nextL}}\\) \\(e_{{nextR}}\\) EdgeData data Winged Edge"},{"location":"mesh/#one-ring-traversal-in-winged-edge","title":"One Ring Traversal in Winged Edge","text":"<ul> <li>Start with a vertex</li> <li>Get one of its edges</li> <li>Add the other vertex of the edge to the one ring</li> <li>Switch to the opposite edge</li> <li>Set <code>curr_edge = ePrevR</code></li> <li>Till <code>curr_edge-&gt;v0</code> is not equal to the first vertex in ring, add it</li> <li>Repeat for <code>ePrevR</code></li> </ul> One Ring Traversal in Winged Edge"},{"location":"mesh/#half-edge","title":"Half Edge","text":"<p>Half edge is a more compact data structure. Each edge is split into two half edges. Each half edge has the following</p> <ul> <li>Vertex reference</li> <li>Face reference (always the one in anti-clockwise direction)</li> <li>Next half edge reference</li> <li>Previous half edge reference</li> <li>Twin half edge reference</li> <li>Half edge data</li> </ul> Vertex Point position HalfEdgeRef edge VertexData data Face HalfEdgeRef edge FaceData data Edge VertexRef vertex FaceRef face HalfEdgeRef prev HalfEdgeRef next HalfEdgeRef twin EdgeData data Half Edge"},{"location":"mesh/#one-ring-traversal-in-half-edge","title":"One Ring Traversal in Half Edge","text":"<ul> <li>Start with a vertex</li> <li>Go to one of its half edges</li> <li>Switch to reverse edge (twin)</li> <li>Go to the next half edge (original vertex)</li> <li>Repeat until you repeat the original edge</li> </ul> One Ring Traversal in Half Edge"},{"location":"mesh/#boundary-traversal-in-half-edge","title":"Boundary Traversal in Half Edge","text":"<ul> <li>Start with a boundary edges</li> <li>Go to the next boundary edge</li> <li>Switch to the reverse edge (twin)</li> <li>Repeat until you reach the original edge</li> </ul> Boundary Traversal in Half Edge Directed Edge <p>Half edge modification for triangular meshes.</p> <ul> <li>Store all 3 half-edges of common face next to each other in memory.</li> <li>Let \\(f\\) be the index of the same face. Place it's \\(k\\)th \\((0,1,2)\\) half-edge at index \\(3f+k\\) in the array.</li> <li>Then \\(h\\)th half-edge belongs to \\(f\\)th face = \\(h / 3\\)</li> <li>Index of \\(h\\)th half-edge in the array = \\(h mod 3\\)</li> <li>No need to store face-to-edge and face-to-edge references.</li> </ul>"},{"location":"mesh/#performance-comparison-of-mesh-data-structures","title":"Performance Comparison of Mesh Data Structures","text":"One Ring, Two Ring, and k-Ring <ul> <li>One Ring: All vertices connected to a vertex by an edge. </li> <li>Two Ring: Vertex connected to a vertex in the one ring. </li> <li>k-Ring: All vertices connected to a vertex by an edge or a vertex connected to a vertex in the k-1 ring.</li> </ul> <p>Roughly speaking, the one ring is the immediate neighbors of a vertex, the two ring is the neighbors of the neighbors, and so on.</p> Data Structure Space per Vertex Mesh Topology Rendering One-Ring Traversal Boundary Traversal Face Set 72 bytes Static, fixed (3,4) Fast Slow Slow Indexed Face Set 36 bytes Static, fixed (3,4) Fast Slow Slow Winged Edge 120 bytes Any (2 manifolds) Medium Slow (case distinctions) Slow Half Edge 144 / 96 bytes Any (2 manifolds) Medium / Slow Fast Fast Directed Edge 64 bytes Regular Triangular / Quad Meshes (2 manifolds) Medium/Slow Medium Medium"},{"location":"mesh/#pros-and-cons-of-mesh-data-structures","title":"Pros and Cons of Mesh Data Structures","text":"Data Structure Pros Cons Face Set Static meshes; rendering No explicit connectivity information; data redundancy Indexed Face Set Simple and efficient storage; Static meshes; rendering No explicit connectivity information; Not efficient for most algorithms Winged Edge Arbitrary Polygonal Meshes Massive case distinctions for one-ring traversal Half Edge One-ring traversal; explicit representation of edges Slow rendering"},{"location":"mesh/#applications-of-mesh-data-structures","title":"Applications of Mesh Data Structures","text":"Data Structure Applications Face Set Stereolithography (3D printing) Indexed Face Set Rendering Winged Edge Rarely used Half Edge Mesh refinement, decimation, smoothing"},{"location":"parametric_curves/","title":"Parametric Curves","text":"<p>Note</p> <p>Please read about Curves before reading this article.</p>"},{"location":"parametric_curves/#re-parametrization-of-curves","title":"Re-Parametrization of Curves","text":"<p>Given a function \\(f(u), u \\in [a, b]\\), defining another function \\(f_2(u) = f(g(u))\\) is called re-parametrization of \\(f\\), where \\(g: \\mathbb{R} \\rightarrow \\mathbb{R}\\).</p> <p>For example, for a curve \\(f\\) with parameter \\(u \\in [0, 1]\\),</p> <ul> <li>\\((x, y) = f(u) = (u, u)\\)</li> <li>\\((x, y) = f(u) = (u^2, u^2)\\)</li> <li>\\((x, y) = f(u) = (u^5, u^5)\\)</li> </ul> <p>They all represent the same curve but with different speeds.</p>"},{"location":"parametric_curves/#arc-length-parametrization","title":"Arc-Length Parametrization","text":"<p>Arc-length distance along a curve from point \\(f(0)\\) to \\(f(v)\\):</p> \\[ s = \\int_o^v \\left|\\frac{df(t)}{dt}\\right| dt \\] <p>\\(s\\) could be used as a natural parameterization for \\(f\\). For \\(f(s)\\), the magnitude of tangent is constant, i.e. \\(|df(s)/ds| = 1\\).</p>"},{"location":"parametric_curves/#curve-representations","title":"Curve Representations","text":"<p>A polynomial function has the form \\(f(t) = a_0 + a_1 t + a_2 t^2 + ... + a_n t^n, a_n \\neq 0\\). where \\(a_i\\) are coefficients and \\(n\\) is the degree of the polynomial. Cannonical form of a polynomial curve becomes \\(f(t) = \\sum_{i=0}^n a_i t^i\\), but this can be generalized to</p> \\[f(t) = \\sum_{i=0}^n c_i B_i(t)\\] <p>where \\(B_i(t)\\) is a polynomial basis function.</p>"},{"location":"parametric_curves/#line-segment","title":"Line Segment","text":"<p>Line segment connection \\(\\mathbf{p}_0\\) and \\(\\mathbf{p}_1\\)</p> \\[ \\begin{align*} f(u) &amp;= \\mathbf{p}_0 + u(\\mathbf{p}_1 - \\mathbf{p}_0) \\\\ &amp;= \\mathbf{a}_0 + u\\mathbf{a}_i \\\\ &amp;= \\mathbf{u} \\cdot \\mathbf{a} \\end{align*} \\] <p>Here, \\(\\mathbf{u} = [1, u]\\) and \\(\\mathbf{a} = [\\mathbf{a}_0, \\mathbf{a}_1]\\). In general, we can write</p> \\[ \\begin{align*} \\mathbf{p}_0 &amp;= f(0) = \\begin{bmatrix} 1 &amp; 0 \\end{bmatrix} \\cdot \\begin{bmatrix} \\mathbf{a}_0 &amp; \\mathbf{a}_1 \\end{bmatrix}^\\top \\\\ \\mathbf{p}_1 &amp;= f(1) = \\begin{bmatrix} 1 &amp; 1 \\end{bmatrix} \\cdot \\begin{bmatrix} \\mathbf{a}_0 &amp; \\mathbf{a}_1 \\end{bmatrix}^\\top \\end{align*} \\] <p>In matrix form, it becomes </p> \\[\\begin{bmatrix} \\mathbf{p}_0 &amp; \\mathbf{p}_1 \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 0 \\\\ 1 &amp; 1 \\end{bmatrix} \\cdot \\begin{bmatrix} \\mathbf{a}_0 &amp; \\mathbf{a}_1 \\end{bmatrix}^\\top \\implies \\mathbf{p} = \\mathbf{C} \\cdot \\mathbf{a}\\] <p>Where \\(\\mathbf{C}\\) is the constraint matrix, and \\(\\mathbf{a}\\) are unknown coefficients. Inversion of \\(\\mathbf{C}\\) gives \\(f(u) = \\mathbf{u} \\mathbf{B} \\mathbf{p}\\), where \\(\\mathbf{B} = \\mathbf{C}^{-1}\\).</p>"},{"location":"parametric_curves/#quadratic-curves","title":"Quadratic Curves","text":"<p>Same cannonical form applies by letting \\(n = 2\\). \\(\\mathbf{B}\\) is a \\(3 \\times 3\\) matrix, obtained by solving a linear system of positional constraints. Constraints on derivatives can be applied as well</p> \\[ \\begin{align*} f(u) = \\mathbf{a}_0 + u\\mathbf{a}_1 + u^2\\mathbf{a}_2 \\\\ f'(u) = \\frac{df(u)}{du} = \\mathbf{a}_1 + 2u\\mathbf{a}_2 \\\\ f''(u) = \\frac{d^2f(u)}{du^2} = 2\\mathbf{a}_2 \\end{align*} \\] <p>Which produces \\(C = \\begin{bmatrix} 1 &amp; u &amp; u^2 \\\\ 0 &amp; 1 &amp; 2u \\\\ 0 &amp; 0 &amp; 2 \\end{bmatrix}\\)</p>"},{"location":"parametric_curves/#cubic-curves","title":"Cubic Curves","text":"<p>Same idea, but now we have 4 constraints. Hermite form is where positional and first derivative constraints are imposed as first (\\(u = 0\\)) and last points (\\(u = 1\\)). Let's work it out through one example.</p> <ul> <li>\\(p_0 = f(0)\\)</li> <li>\\(p_1 = f'(0)\\)</li> <li>\\(p_2 = f(1)\\)</li> <li>\\(p_3 = f'(1)\\)</li> </ul> <p>Now we know that \\(\\mathbf{p} = \\mathbf{C} \\cdot \\mathbf{a}\\) where \\(\\mathbf{C}\\) is our constraint matrix and \\(\\mathbf{a}\\) is the unknown coefficients. We will consider each constraint one by one.</p> <ul> <li>\\(p_0 = f(0) = a_0 \\implies\\) first row of \\(\\mathbf{C}\\) is \\([1, 0, 0, 0]\\)</li> <li>\\(p_1 = f'(0) = a_1 \\implies\\) second row of \\(\\mathbf{C}\\) is \\([0, 1, 0, 0]\\)</li> <li>\\(p_2 = f(1) = a_0 + a_1 + a_2 + a_3 \\implies\\) third row of \\(\\mathbf{C}\\) is \\([1, 1, 1, 1]\\)</li> <li>\\(p_3 = f'(1) = a_1 + 2a_2 + 3a_3 \\implies\\) fourth row of \\(\\mathbf{C}\\) is \\([0, 1, 2, 3]\\)</li> </ul> <p>So our matrix \\(\\mathbf{C}\\) becomes</p> \\[\\mathbf{C} = \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 1 &amp; 2 &amp; 3 \\end{bmatrix}\\] <p>Inverting this, we get \\(\\mathbf{B} = \\mathbf{C}^{-1}\\), and our curve becomes \\(f(u) = \\mathbf{u} \\mathbf{B} \\mathbf{p}\\).</p>"},{"location":"parametric_curves/#blending-functions","title":"Blending Functions","text":"<p>Define a vector of functions \\(\\mathbf{b}(u) = \\mathbf{u} \\mathbf{B}\\), where elements of \\(\\mathbf{b}(u)\\) are blending functions. The control points can be blended linearly with these functions to get the curve</p> \\[f(u) = \\sum_{i=0}^n \\mathbf{b}_i(u) \\mathbf{p}_i\\]"},{"location":"parametric_curves/#piecewise-polynomial-curves","title":"Piecewise Polynomial Curves","text":"\\[f(u) = \\begin{cases} f_1(2u) &amp; u \\in [0, 0.5] \\\\ f_2(2u-1) &amp; u \\in [0.5, 1] \\end{cases}\\] <p>where \\(f_1\\) and \\(f_2\\) are functions for each of the two line segments.  These curves are only \\(C^0\\) continuous, i.e. they are continuous but not smooth.</p> <p>we can also write piecewise polynomials as a weighted sum of basis functions</p> \\[f(u) = p_1 b_1(u) + p_2 b_2(u) + p_3 b_3(u)\\]"},{"location":"parametric_curves/#knots","title":"Knots","text":"<p>In a piecewise function, knots are sites where pieces begins or ends. We can represent the knots by their \\(u\\) values, and store them in a vector in ascending order. Such a vector is called a Knot vector</p>"},{"location":"parametric_curves/#cubics","title":"Cubics","text":"<p>They allow for \\(C^2\\) continuity, which is considered suitable for most visual tasks. They provide a minimum-curvature interpolants to a set of points</p> Desirable Property B-Splines Catmull-Rom Cardinal Natural Cubics Piece-wise cubic Yes Yes Yes Yes Interpolating curve No Yes Yes Yes Curve has local control Yes Yes Yes No Curve has \\(C^2\\) continuity Yes No No Yes <p>Can only satisfy 3 of the 4 properties at a time.</p>"},{"location":"parametric_curves/#natural-cubic-splines","title":"Natural Cubic Splines","text":"<p>For one segment, parameterized by the positions on it's end-points, and the first and second derivative at the beginning point</p> <ul> <li>\\(p_0 = f(0)\\)</li> <li>\\(p_1 = f'(0)\\)</li> <li>\\(p_2 = f''(0)\\)</li> <li>\\(p_3 = f(1)\\)</li> </ul> <p>The constraint matrix \\(\\mathbf{C}\\) becomes \\(\\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 2 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 &amp; 1 \\end{bmatrix}\\)</p>"},{"location":"parametric_curves/#hermite-cubics","title":"Hermite Cubics","text":"<p>For one segment, parameterized by the positions on it's end-points, and the first derivative at end-points </p> <ul> <li>\\(p_0 = f(0)\\)</li> <li>\\(p_1 = f'(0)\\)</li> <li>\\(p_2 = f(1)\\)</li> <li>\\(p_3 = f'(1)\\)</li> </ul> <p>We did this earlier, refer to the polynomial section for the constraint matrix.</p>"},{"location":"parametric_curves/#cardinal-cubic-splines","title":"Cardinal Cubic Splines","text":"<p>Interpolates all but the first and last points. Tension parameter controls the tightness of the curve. For \\(t = 0\\), it becomes a Catmull-Rom spline. It's local, but only \\(C^1\\) continuous.</p> <p>Each segment uses 4 control points \\(i, i+1, i+2, i+3\\). Segment begins at second point \\(p_1\\) and ends at third point \\(p_2\\). Derivative at \\(p_1\\) is proportional to vector \\(p_2 - p_0\\) and derivative at \\(p_2\\) is proportional to \\(p_3 - p_1\\). </p> <ul> <li>\\(f(0) = p_1\\)</li> <li>\\(f(1) = p_2\\)</li> <li>\\(f'(0) = \\frac{1}{2}(1 - t)(p_2 - p_0)\\)</li> <li>\\(f'(1) = \\frac{1}{2}(1 - t)(p_3 - p_1)\\)</li> </ul> <p>Solving this to find \\(p_0\\) and \\(p_3\\), we get</p> <ul> <li>\\(p_0 = f(1) - \\frac{2}{1-t}f'(0)\\)</li> <li>\\(p_1 = f(0)\\)</li> <li>\\(p_2 = f(1)\\)</li> <li>\\(p_3 = f(0) + \\frac{2}{1-t}f'(1)\\)</li> </ul> <p>Note</p> <p>Do work out the matrix \\(\\mathbf{C}\\) for this case.</p>"},{"location":"parametric_curves/#approximating-curves","title":"Approximating Curves","text":"<p>Curves which donot pass through the control points are called approximating curves. </p>"},{"location":"parametric_curves/#bezier-curves","title":"Bezier Curves","text":"<p>We have already studied bezier curves, but let's use polynomial representation to understand them better.</p> <ul> <li>\\(p_0 = f(0)\\)</li> <li>\\(p_3 = f(1)\\)</li> <li>\\(3(p_1 - p_0) = f'(0)\\)</li> <li>\\(3(p_3 - p_2) = f'(1)\\)</li> </ul> <p>Rewriting it in terms of blending functions, we get </p> \\[f(u) = \\sum_{i=0}^3 \\mathbf{b}_i(u) \\mathbf{p}_i\\] <p>with blending functions \\(\\mathbf{b}_i(u) = \\binom{3}{i} u^i (1-u)^{3-i}\\). These blending functions are also called Bernstein polynomials.</p> <p>A way to evaluate the curve is to use de Casteljau's algorithm.</p> <p>Subdivide each line segment of the control polyline in the ratio t:(1-t) and join to create new poly line. Continue until only 1 point is left.This point lies on the B\u00e9zier curve at parameter t.</p> De Casteljau's Algorithm"},{"location":"parametric_curves/#b-splines","title":"B-Splines","text":"<p>Approximating a set of \\(n\\) points with a polynomial of degree \\(d\\) that gives \\(C^{d-1}\\) continuity. Generally, </p> \\[f(t) = \\sum_{i = 1}^n \\mathbf{b}_i(t) \\mathbf{p}_i\\] <p>where \\(\\mathbf{b}_i(t)\\) are blending functions. The curve is defined by a set of control points \\(\\mathbf{p}_i\\) and a set of blending functions \\(\\mathbf{b}_i(t)\\). The blending functions are defined by a set of knots \\(t_i\\).</p>"},{"location":"parametric_curves/#uniform-linear-b-splines","title":"Uniform linear B-Splines","text":"\\[b_{i, 2} = \\begin{cases} t - i &amp; i \\leq t &lt; i+1 \\\\  2 - t + i &amp; i+1 \\leq t &lt; i+2 \\\\ 0 &amp; \\text{otherwise} \\end{cases}\\]"},{"location":"parametric_curves/#uniform-quadratic-b-splines","title":"Uniform Quadratic B-Splines","text":"\\[b_{i, 3} = \\begin{cases} \\frac{1}{2}(t - i)^2 &amp; i \\leq t &lt; i+1 \\\\ \\frac{1}{2} + (t - i)(2 - t + i) &amp; i+1 \\leq t &lt; i+2 \\\\ \\frac{1}{2}(2 - t + i)^2 &amp; i+2 \\leq t &lt; i+3 \\\\ 0 &amp; \\text{otherwise} \\end{cases}\\]"},{"location":"parametric_curves/#non-uniform-b-splines","title":"Non-Uniform B-Splines","text":"<p>B-splines are defined for any non-decreasing knot vector \\(\\mathbf{t}\\), which not necessarily be Uniform. Non-uniform knot-spacing actually gives more control over the shape of the curve.</p>"},{"location":"rasterization/","title":"Rasterization","text":"<p>Rasterization is the process of converting vector graphics into raster graphics. It is a key step in the rendering pipeline, and is responsible for converting the 3D scene into a 2D image that can be displayed on a screen. It consists of </p> <ul> <li>Enumerating the pixels that are covered by each primitive in the scene</li> <li>Interpolating the attributes of the primitive across the pixels</li> <li>Output a set of fragments for each pixel, which will be shaded in the next stage of the pipeline. Fragment contains the interpolated attributes of the primitive, such as color, depth, and texture coordinates.</li> </ul> Rasterization"},{"location":"rasterization/#rasterizing-lines","title":"Rasterizing Lines","text":""},{"location":"rasterization/#dda-algorithm","title":"DDA Algorithm","text":"\\[ m = \\frac{dy}{dx} = \\frac{y_1 - y_0}{x_1 - x_0} = \\frac{\\Delta y}{\\Delta x} \\] <p>For \\(\\Delta x\\) change in x-coordinate, change in y-coordinate is \\(\\Delta y = m \\Delta x\\).</p>"},{"location":"rasterization/#midpoint-line-algorithm","title":"Midpoint Line Algorithm","text":"<ul> <li>Drawing lines using implicit equation of line \\(f(x, y) = (y_0 - y_1)x + (x_1 - x_0)y + x_0y_1 - x_1y_0 = 0\\).</li> <li>Assume \\(x_0 &lt; x_1\\). If not, swap points.</li> <li>\\(m = \\frac{y_1 - y_0}{x_1 - x_0}\\)</li> </ul> <p>Basic idea is the following</p> <ul> <li>Next potential pixel is either \\((x + 1, y)\\) or \\((x + 1, y + 1)\\).</li> <li>Midpoint : \\((x + 1, y + 0.5)\\)</li> <li>If line passes below midpoint, choose \\((x + 1, y)\\), else choose \\((x + 1, y + 1)\\).</li> </ul> <pre><code>y = y_0\nfor x = x_0 to x_1:\n    draw(x, y)\n    if f(x + 1, y + 0.5) &lt; 0:\n        y = y + 1\n</code></pre> Midpoint Line Algorithm <p>More efficient version would be to reuse computation. Inside the loop, one of these is already evaluated in the last step: \\(f(x - 1, y + 0.5)\\), \\(f(x - 1, y - 0.5)\\). Use the relation</p> \\[f(x + 1, y) = f(x, y) + (y_0 - y_1)\\] \\[f(x + 1, y + 1) = f(x, y) + (y_0 - y_1) + (x_1 - x_0)\\] <pre><code>y = y_0\nd = 2(y_0 - y_1)(x_0 + 1) + (x_1 - x_0)(2y_0 + 1) + 2(x_0y_1 - x_1y_0)\n\nfor x = x_0 to x_1:\n    draw(x, y)\n    if d &lt; 0:\n        y = y + 1\n        d += 2(x_1 - x_0)\n    d += 2(y_0 - y_1)\n</code></pre>"},{"location":"rasterization/#rasterizing-circles-midpoint-circle-algorithm","title":"Rasterizing Circles - Midpoint Circle Algorithm","text":"<ul> <li>Drawing circles using implicit equation of circle \\(f(x, y) = x^2 + y^2 - r^2 = 0\\).</li> <li>Draw octant of a circle and reflect it to get the full circle.</li> </ul> <pre><code>x = 0\ny = r\nd0 = (1, r - 1/2) = 1 + (r - 1/2)^2 - r^2 = 5/4 - r\nd0 = round(d0) = 1 - r if r is integer\n\nwhile x &lt; y:\n    draw(x, y)\n    x += 1\n    if d &lt; 0:\n        d += 2x + 3\n    else:\n        d += 2(x - y) + 5\n        y -= 1\n</code></pre> Midpoint Circle Algorithm"},{"location":"rasterization/#rasterizing-triangles","title":"Rasterizing Triangles","text":"<ul> <li>Drawing a 2D triangle with points \\(a(x_a, y_a), b(x_b, y_b), c(x_c, y_c)\\).</li> <li>Color interpolation using barycentric coordinates. \\(c = \\alpha c_0 + \\beta c_1 + \\gamma c_2\\) where \\(\\alpha + \\beta + \\gamma = 1\\).</li> <li>Rasterize adjacent triangles so that there are no holes</li> </ul>"},{"location":"rasterization/#barycentric-coordinates","title":"Barycentric Coordinates","text":"<p>For a point P inside a triangle, \\(P = \\alpha P_0 + \\beta P_1 + \\gamma P_2\\) where \\(\\alpha + \\beta + \\gamma = 1, \\alpha, \\beta, \\gamma \\geq 0\\).</p> \\[f_{ac}(x, y) = (y_a - y_c)x + (x_c - x_a)y + x_ay_c - x_cy_a = 0\\] <p>We can find value of \\(\\alpha, \\beta, \\gamma\\) as follows</p> \\[ \\begin{align*} \\beta &amp;= \\frac{f_{ac}(x, y)}{f_{ac}(x_b, y_b)} \\\\ \\gamma &amp;= \\frac{f_{ab}(x, y)}{f_{ab}(x_c, y_c)} \\\\ \\alpha &amp;= 1 - \\beta - \\gamma \\end{align*} \\] <pre><code>def barycentric(a, b, c, x, y):\n    beta = f_ab(x, y) / f_ab(x_c, y_c)\n    gamma = f_ac(x, y) / f_ac(x_b, y_b)\n    alpha = 1 - beta - gamma\n\n    return alpha, beta, gamma\n\nx_min, y_min, x_max, y_max = bounding_box(a, b, c)\n\nfor y in range(y_min, y_max):\n    for x in range(x_min, x_max):\n        alpha, beta, gamma = barycentric(a, b, c, x, y)\n        if alpha &gt;= 0 and beta &gt;= 0 and gamma &gt;= 0:\n            c = alpha * c_a + beta * c_b + gamma * c_c\n            draw(x, y) with color c\n</code></pre> <p>What about boundary pixels?</p> <p>In order to ensure no gaps remain between adjacent triangles,</p> <ul> <li>Either choose to draw the edge of one of the triangles (harder to implement)</li> <li>Or draw the edge of both triangles (easier to implement)</li> </ul>"},{"location":"ray_tracing/","title":"Ray Tracing","text":"<p>Rendering is a process that takes as inputs a set of objects and produces an array of pixels. It can be organized in 2 ways</p> <ul> <li>Object-order rendering: Where the objects are processed one by one</li> <li>Image-order rendering: Where the pixels are processed one by one</li> </ul> <p>Ray tracing is an image-order algorithm</p>"},{"location":"ray_tracing/#basic-algorithm","title":"Basic Algorithm","text":"<p>A basic ray tracer consists of three parts </p> <ul> <li>Ray generation : Computes origin and direction of each pixel's viewing ray based on camera geometry</li> <li>Ray intersection : Finds the intersection of the viewing ray with the scene geometry</li> <li>Lighting : Computes the color of the pixel based on the result of ray-object intersection</li> </ul> <pre><code>for each pixel in the image:\n    compute viewing ray\n    find first object hit by the ray and its surface normal n\n    shade the pixel using hit-point, light and n\n</code></pre>"},{"location":"ray_tracing/#ray-generation-or-computing-viewing-ray","title":"Ray Generation or Computing Viewing Ray","text":"<ul> <li>basic tools for ray generation are the viewpoint and the image plane</li> <li>Ray representation \\(p(t) = e + t (s - e)\\), where \\(e\\) is eye and \\(s\\) is a point on the image plane</li> <li>Point \\(e\\) is ray's origin and (s - e) is ray's direction</li> </ul> Ray Generation <ul> <li>Ray generation takes place from the camera frame</li> <li>position \\(e\\) (eye point)</li> <li>\\(\\mathbf{u},  \\mathbf{v},  \\mathbf{w}\\) are basis vectors</li> <li>Choose \\(-w\\) as view direction and an up vector, using which we construct the basis vectors</li> </ul> Camera Vectors"},{"location":"ray_tracing/#orthographic-views","title":"Orthographic Views","text":"<ul> <li>All rays will have direction \\(- \\mathbf{w}\\)</li> <li>A viewpoint is not require but viewing rays can start from the plane containing the camera, so we know when object is behind the camera.</li> <li>Viewing rays start on the image plane and are parallel to each other, and cab be defined by point \\(e\\) and vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\)</li> </ul> Orthographic Rays <ul> <li>\\(l\\) and \\(r\\) are left and right limits of the image plane (measured along \\(\\mathbf{u}\\)) \\(l &lt; 0 &lt; r\\)</li> <li>\\(b\\) and \\(t\\) are bottom and top limits of the image plane (measured along \\(\\mathbf{v}\\)) \\(b &lt; 0 &lt; t\\)</li> </ul> <p>Pixel spacing for \\(n_x \\times n_y\\) image is</p> <ul> <li>Horizontal : \\(\\Large \\frac{r - l}{n_x}\\)</li> <li>Vertical : \\(\\Large \\frac{t - b}{n_y}\\)</li> </ul> <p>Therefore, pixel position \\((i, j)\\) in the raster image is </p> <ul> <li>\\(u = l + \\Large \\frac{(r - l)(i + 0.5)}{n_x}\\)</li> <li>\\(v = b + \\Large \\frac{(t - b)(j + 0.5)}{n_y}\\)</li> </ul> <p>Where \\(u, v\\) are the coordinates of the pixel in the image plane w.r.t origin \\(e\\) and basis vectors \\(\\mathbf{u} ,  \\mathbf{v}\\)</p> <p>Therefore, Ray parameters are </p> <ul> <li>Origin: \\(e + u * \\mathbf{u} + v * \\mathbf{v}\\)</li> <li>Direction : \\(-\\mathbf{w}\\)</li> </ul>"},{"location":"ray_tracing/#perspective-views","title":"Perspective Views","text":"<ul> <li>All rays will have different directions for different pixels</li> <li>All rays will have origin at the eye point</li> </ul> Perspective Rays <ul> <li>Image plane positioned at distance \\(d\\) from the eye point \\(e\\)</li> <li>Ray parameters are <ul> <li>Origin: \\(e\\)</li> <li>Direction: \\(-d * \\mathbf{w} + u * \\mathbf{u} + v * \\mathbf{v}\\)</li> </ul> </li> </ul> <p>Where \\(u, v\\) are the coordinates of the pixel in the image plane w.r.t origin \\(e\\) and basis vectors \\(\\mathbf{u} ,  \\mathbf{v}\\) </p> <p>Note</p> <p>In some sense, the origin in orthographic view is direction in perspective view.</p>"},{"location":"ray_tracing/#ray-intersection","title":"Ray intersection","text":"<ul> <li>Given a generated ray \\(p(t) = e + t \\cdot d\\), find intersection (first hit) with the scene geometry such that \\(t &gt; 0\\)</li> <li>Given a ray \\(p(t) = e + t \\cdot d\\), and an implicit surface \\(f(p) = 0\\), the intersection point is found by solving \\(f(e + t \\cdot d) = 0\\) </li> <li>There can be multiple solutions for \\(t\\), but we are interested in the smallest positive solution</li> </ul>"},{"location":"ray_tracing/#for-sphere","title":"For Sphere","text":"<ul> <li>Parametric equation for any point \\(p\\) on a sphere with center \\(c\\) and radius \\(r\\) is </li> </ul> \\[ (p - c) \\cdot (p - c) - r^2 = 0 \\] <ul> <li>For finding points of intersection of a ray with a sphere, we substitute \\(p = e + t \\cdot d\\) in the above equation and solve for \\(t\\)</li> </ul> \\[ \\begin{align*}     (e + t \\cdot d - c) \\cdot (e + t \\cdot d - c) - r^2 &amp;= 0 \\\\     (d \\cdot d) t^2 + 2 (d \\cdot (e - c)) t + (e - c) \\cdot (e - c) - r^2 &amp;= 0 \\end{align*} \\] <p>This gives the solution</p> \\[t = \\frac {-(d \\cdot (e - c)) \\pm \\sqrt{(d \\cdot (e - c))^2 - (d \\cdot d) ((e - c) \\cdot (e - c) - r^2)}}{d \\cdot d}\\]"},{"location":"ray_tracing/#normal-for-sphere","title":"Normal for Sphere","text":"<ul> <li>The normal vector at \\(\\mathbf{p}\\) on the implicit surface \\(f(p)\\) is given by </li> </ul> \\[n = \\nabla f(p) = (\\frac{\\partial f(p)}{\\partial x}, \\frac{\\partial f(p)}{\\partial y}, \\frac{\\partial f(p)}{\\partial z})\\] <ul> <li>For sphere, the normal at point \\(p\\) is</li> </ul> \\[n = 2(p - c) \\hspace{20px} \\text{and} \\hspace{20px} \\hat{n} = \\frac{p - c}{R}\\]"},{"location":"ray_tracing/#for-triangle","title":"For Triangle","text":"<ul> <li>Parametric equation for a triangle is</li> </ul> \\[\\mathbf{e} + t \\mathbf{d} = \\mathbf{f}(u, v)\\] <ul> <li> <p>3 unknowns \\(t, u, v\\) and 3 equations \\((x, y, z)\\) for the triangle. We can solve for \\(t, u, v\\) and check if \\(u, v\\) are within the range of the triangle.</p> </li> <li> <p>For a parametric plane, the parametric surface can be represented in terms of any three points in the plane. Utilize barycentric coordinates for ray-triangle test </p> </li> <li> <p>For a triangle with vertices \\(\\mathbf{a},  \\mathbf{b}, \\mathbf{c}\\) intersection will occur when </p> </li> </ul> \\[\\mathbf{e} + \\mathbf{d} t = \\mathbf{a} + \\beta (\\mathbf{b} - \\mathbf{a}) + \\gamma (\\mathbf{c} - \\mathbf{a})\\] <ul> <li>Intersection is inside the triangle if \\(\\beta, \\gamma \\geq 0\\) and \\(\\beta + \\gamma \\leq 1\\)</li> <li>To solve for \\(t, \\beta, \\gamma\\), we can write the above equation as</li> </ul> \\[ \\begin{align*}     x_e + t x_d &amp;= x_a + \\beta (x_b - x_a) + \\gamma (x_c - x_a) \\\\     y_e + t y_d &amp;= y_a + \\beta (y_b - y_a) + \\gamma (y_c - y_a) \\\\     z_e + t z_d &amp;= z_a + \\beta (z_b - z_a) + \\gamma (z_c - z_a) \\end{align*} \\] <p>This can be written in matrix form as</p> \\[ \\begin{bmatrix}     x_a - x_b &amp; x_a - x_c &amp; x_d \\\\     y_a - y_b &amp; y_a - y_c &amp; y_d \\\\     z_a - z_b &amp; z_a - z_c &amp; z_d \\end{bmatrix} \\begin{bmatrix}     \\beta \\\\     \\gamma \\\\     t \\end{bmatrix} =  \\begin{bmatrix}     x_a - x_e \\\\     y_a - y_e \\\\     z_a - z_e \\end{bmatrix} \\] <ul> <li> <p>Let \\(A\\) be the matrix on the left, \\(B\\) be the matrix on the right, then using Cramer's rule, we can solve for \\(\\beta, \\gamma, t\\) as follows</p> </li> <li> <p>Let \\(A_i\\) be the matrix obtained by replacing \\(i^{th}\\) column of \\(A\\) with the column of \\(B\\), then values of \\(\\beta, \\gamma, t\\) are</p> </li> </ul> \\[ \\beta = \\frac{\\text{det}(A_1)}{\\text{det}(A)} \\hspace{20px} \\gamma = \\frac{\\text{det}(A_2)}{\\text{det}(A)} \\hspace{20px} t = \\frac{\\text{det}(A_3)}{\\text{det}(A)} \\]"},{"location":"ray_tracing/#ray-polygon-intersection","title":"Ray-Polygon Intersection","text":"<p>Given \\(m\\) vertices \\(p_1, p_2, \\ldots, p_m\\) of a polygon and surface normal \\(n\\) - Compute the intersection point between ray \\(\\mathbf{e} + t \\mathbf{d}\\) and the plane containing the polygon \\((p - p_1) \\cdot n = 0\\)</p> \\[t = \\frac{(p_1 - e) \\cdot n}{d \\cdot n}\\] <ul> <li>If \\(\\mathbf{p}\\) is inside the polygon, then the ray hits it.</li> </ul>"},{"location":"ray_tracing/#point-in-polygon-test","title":"Point-in-Polygon Test","text":""},{"location":"ray_tracing/#ray-polygon-intersection-or-crossing-number-algorithm","title":"Ray Polygon Intersection or Crossing Number Algorithm","text":"<ul> <li>Send a 2D ray out from \\(p\\) and count the number of intersections between the ray and the polygon</li> <li>If the intersection count is odd, then \\(p\\) is inside the polygon</li> </ul> <p>Warning</p> <p>It's a ray, so only one way</p> <ul> <li>Easily done by checking the sign of the cross product of the vectors from \\(p\\) to the vertices of the polygon</li> </ul> Crossing Number Algorithm"},{"location":"ray_tracing/#winding-number-algorithm","title":"Winding Number Algorithm","text":"<ul> <li>Winding number of a closed curve in the plane around a given point is an integer representing the total number of times that curve travels counterclockwise around the point</li> <li>If the winding number is non-zero, then the point is inside the polygon</li> </ul> Winding Number Algorithm"},{"location":"ray_tracing/#shading-and-lighting","title":"Shading and Lighting","text":""},{"location":"ray_tracing/#intersection-with-multiple-objects","title":"Intersection with Multiple Objects","text":"<p>Typically, a scene will contain multiple objects. To find the first object hit by the ray, we need to find the smallest positive \\(t\\) value</p> <pre><code>t_min = infinity\nfirstSurface = None\nfor each object in the surfaceList:\n    hitSurface, t = object.intersect(ray)\n    if hitSurface != None and t &lt; t_min:\n        t_min = t\n        firstSurface = hitSurface\n\nreturn firstSurface, t_min\n</code></pre>"},{"location":"ray_tracing/#shading","title":"Shading","text":"<p>We have already read about the Phong reflection model in the Lighting section. We can use the same model to shade the pixel, so our algorithm becomes as follows</p> <pre><code>def Scene::trace(ray, t_min, t_max):\n    surface, t = surfaces.intersect(ray, t_min, t_max)\n    if surface == None:\n        return background_color\n    else:\n        return surface.shade(ray, t, light)\n\ndef Surface::shade(ray, t, light):\n    p = ray.origin + t * ray.direction\n    n = surface_normal(p)\n    v = normalize(ray.origin - p)\n    l = normalize(light.position - p)\n    h = normalize(v + l)\n    # compute ambient, diffuse and specular components\n</code></pre> <p>The shading algorithm can be changed for multiple light sources as well</p>"},{"location":"ray_tracing/#casting-shadows","title":"Casting Shadows","text":"<ul> <li>Surface is illuminated if nothing blocks the view of the light source</li> <li>To check if a point is in shadow, we can cast a ray from the point to the light source      and check if it intersects any object</li> <li>As many rays need to be cast as there are light sources</li> <li>Ideally test \\(t \\in [0, \\infty]\\), but to account for floating point errors, test \\(t \\in [0 + \\epsilon, \\infty]\\) where \\(\\epsilon\\) is a small positive number</li> </ul> <pre><code>def Surface::shade(ray, t, lights):\n    p = ray.origin + t * ray.direction\n    n = surface_normal(p)\n    v = normalize(ray.origin - p)\n    for each light in lights:\n        l = normalize(light.position - p)\n        h = normalize(v + l)\n        shadowRay = Ray(p, l)\n        if inShadow(shadowRay):\n            # point is in shadow\n            return ambient\n        else:\n            # compute ambient, diffuse and specular components\n            return shading\n</code></pre>"},{"location":"ray_tracing/#specular-reflections","title":"Specular Reflections","text":"<ul> <li>Mirror reflections can be added by shading reflected rays \\(r = d - 2(d \\cdot n)n\\)</li> <li>Some energy is lost in each reflection, so we can recursively </li> </ul> \\[ c = c + k_m \\text{raycolor}(p + tr, \\epsilon, \\infty)\\] <ul> <li>\\(k_m\\) is specular RGB color for mirror reflection</li> <li>Trace reflected rays for materials that are specular</li> <li>Recursion depth can be limited to avoid infinite recursion</li> </ul>"},{"location":"ray_tracing/#refraction-and-transparency","title":"Refraction and Transparency","text":"<ul> <li>Compute refracted ray as following</li> </ul> \\[t = \\frac{n_1}{n_2} (d - (d \\cdot n)n) - n \\sqrt{1 - (\\frac{n_1}{n_2})^2 (1 - (d \\cdot n)^2)}\\]"},{"location":"ray_tracing/#total-internal-reflection","title":"Total Internal Reflection","text":"<ul> <li>Happens when light travels from denser to rarer medium at an angle greater than the critical angle</li> <li>Critical angle \\(\\cos (\\theta_c) = \\frac{n_2}{n_1}\\)</li> </ul>"},{"location":"ray_tracing/#schlicks-approximation","title":"Schlick's Approximation","text":"<ul> <li>Approximates the Fresnel equations for reflection and refraction</li> </ul> \\[R = R_0 + (1 - R_0) (1 - \\cos \\theta)^5, \\hspace{20px} R_0 = \\left( \\frac{n_1 - n_2}{n_1 + n_2} \\right)^2\\]"},{"location":"ray_tracing/#beers-law","title":"Beer's Law","text":"<ul> <li>For homogeneous impurities in a dielectric, a light carrying ray's intensity attenuates as per Beer's law</li> <li>Loss of intensity in the medium \\(dI = -C I dx\\)</li> <li>Solution: \\(I = k' + k e^{-C x}\\) with boundary conditions \\(I(s) = I_0 e^{s\\ln(a)}\\)</li> <li>\\(s\\) is distance from interface, and \\(a\\) is attenuation constant</li> </ul>"},{"location":"ray_tracing/#putting-it-all-together","title":"Putting it all together","text":"<pre><code>def Surface::shade(ray, point, normal, lights):\n    if point is on a dielectric:\n        r = reflect(ray, normal)\n        if dot(ray, normal) &lt; 0: # entering a dielectric\n            c = -dot(ray, normal)\n            k_r = k_g = k_b = 1\n        else:\n            # apply Beer's law\n            k_r = expo(-a_r * t)\n            k_g = expo(-a_g * t)\n            k_b = expo(-a_b * t)\n\n            if refract(ray, -normal, 1/eta, t):\n                c = dot(t, normal)\n            else:\n                return k * Scene::trace(r, point, epsilon, infinity)\n\n        R_0 = ((eta - 1) / (eta + 1)) ** 2\n        R = R_0 + (1 - R_0) * (1 - c) ** 5\n        return k * (\n                R * Scene::trace(r, point, epsilon, infinity) + \\\n                (1 - R) * Scene::trace(t, point, epsilon, infinity)\n            )\n</code></pre>"},{"location":"surfaces_volumes/","title":"Surfaces and Volumes","text":""},{"location":"surfaces_volumes/#surface-representation","title":"Surface Representation","text":"<ul> <li>Formally: Surface is a 2D manifold embedded in 3D space.</li> <li>Informally: Surface is boundary of a non-degenerate 3D solid.</li> </ul> <p>A non-degenerate solid object is one where each point can be classified as either interior or exterior.</p> <p>What is a manifold?</p> <p>An n-dimensional manifold will look like an n-dimensional Euclidean space when looked at closely. For example, if you zoom in infinitely on a hollow sphere, it will look like a 2D plane.</p>"},{"location":"surfaces_volumes/#surface-classifications","title":"Surface Classifications","text":"Classification Examples Orientable/Non Orientable Torus is orientable, Klien Bottle is not Closed/Open Sphere is closed, Plane is open Manifold/Non-Manifold Sphere is manifold, Cone is non-manifold"},{"location":"surfaces_volumes/#topological-classification","title":"Topological Classification","text":"<ul> <li>Topological Equivalence: Two surfaces are topologically equivalent if one can be deformed into the other without any cuts or gluing.</li> <li>Genus: Maximum Number of cuttings along non-intersecting closed curves that can be made without separating the surface.</li> </ul>"},{"location":"surfaces_volumes/#operational-classification","title":"Operational Classification","text":"<ul> <li>Evaluation: The sampling of the surface geometry or other surface properties.</li> <li>Modification: A surface can be changed in terms of geometry(deformations) or topology(cuts, gluing).</li> <li>Query: Spacial queries to determine if given point is inside or outside the solid bounded by the surface, or distance of point from the surface.</li> </ul>"},{"location":"surfaces_volumes/#surface-representation_1","title":"Surface Representation","text":"<ul> <li>Implicit Surfaces: Defined as the set of points satisfying an equation \\(f(x, y, z) = 0\\).</li> <li>Parametric Surfaces: \\(S(u, v) = (x(u, v), y(u, v), z(u, v))\\) where \\(S: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3\\).</li> </ul>"},{"location":"surfaces_volumes/#volume-representation","title":"Volume Representation","text":""},{"location":"surfaces_volumes/#uniform-grid","title":"Uniform Grid","text":"<ul> <li>Trivial 3D grid where each cell is a cube, and it stores certain data, like color, density, etc. Used in medical imaging, many GPU applications, etc.</li> </ul> <p>Construction:</p> <ul> <li>Find the bounding box of the object.</li> <li>Define grid resolution (manual or automatic).</li> <li>Choose indexing and create huge arrays in memory</li> <li>For each cell, sample desired values and store them in cell</li> <li>Huge memory overhead!</li> </ul> <p>Pros</p> <ul> <li>Trivial to implement</li> <li>Very parallelizable</li> <li>Trivial boolean operations</li> </ul> <p>Cons</p> <ul> <li>Huge memory overhead</li> <li>large 3D loops make algorithms too slow</li> </ul> Z-Order Curve / Z Indexing <p>This is a traversal method for 3D grids, where the cells are traversed in a specific order, which provides better cache coherency and locality.</p> <p> Z-Order Curve </p>"},{"location":"surfaces_volumes/#octree","title":"Octree","text":"<p>Adaptive grid structure where cells are recursively subdivided until a certain criterion is met. Each non-leaf node has 8 half-sized children. Used in Volume data storage, Color quantization, Collision detection, etc.</p> Octree"},{"location":"surfaces_volumes/#data-structure","title":"Data structure","text":"<ul> <li>Node<ul> <li><code>NodeType</code> type</li> <li><code>NodeRef subNodes[8]</code></li> </ul> </li> <li>NodeType<ul> <li><code>EMPTY</code> - all children are empty</li> <li><code>MIXED</code> - atleast one non-empty subcell</li> <li><code>FULL</code> - all children are full</li> </ul> </li> </ul>"},{"location":"surfaces_volumes/#construction","title":"Construction","text":"<ul> <li>Top down approach:<ul> <li>Fit whole data(geometry) into one bounding cell</li> <li>If it is mixed \\(\\rightarrow\\) subdivide into 8 children</li> <li>Repeat recursively until there is nothing more to subdivide</li> </ul> </li> <li>Bottom up approach:<ul> <li>Create uniform grid with high resolution</li> <li>For each 8 neighbours do:<ul> <li>If all are empty, merge them</li> <li>If all are full, merge them</li> <li>Else, create mixed parent cell and continue</li> </ul> </li> </ul> </li> </ul> <p>Pros</p> <ul> <li>Memory efficient</li> <li>Fast boolean operations</li> <li>Adaptive resolution</li> </ul> <p>Cons</p> <ul> <li>Longer point localization (data search)</li> <li>Small changes in geometry can lead to large changes in tree</li> </ul>"},{"location":"texture_mapping/","title":"Texture Mapping","text":"<p>Objects have variation in reflectance across surface. A common technique to handle such variation is to store the reflectance as a function or a pixel-based image and \"map\" it onto the surface. This function or image is called Texture, and the process of controlling reflectance properties of objects using textures is called Texture Mapping.</p>"},{"location":"texture_mapping/#2d-texture-mapping","title":"2D Texture Mapping","text":"<ul> <li>Local 2D coordinate system called uv is used to create reflectance \\(R(u,v)\\).</li> <li>During texture mapping, each surface point \\((x,y,z)\\) is assigned a 2D coordinate \\((u,v)\\) and colored with \\(R(u,v)\\).</li> </ul>"},{"location":"texture_mapping/#texture-mapping-on-sphere","title":"Texture mapping on sphere","text":"<ul> <li>To map \\((u, v)\\) onto a sphere, we first compute spherical coordinate<ul> <li>\\(x = x_c + r \\sin(\\theta) \\cos(\\phi)\\)</li> <li>\\(y = y_c + r \\sin(\\theta) \\sin(\\phi)\\)</li> <li>\\(z = z_c + r \\cos(\\theta)\\)</li> <li>\\(\\theta = arccos(\\frac{z - z_c}{r})\\)</li> <li>\\(\\phi = arctan2(y - y_c, x - x_c)\\) </li> </ul> </li> <li>Required mapping becomes<ul> <li>\\(u = \\frac{\\phi + \\pi}{2\\pi}\\)</li> <li>\\(v = \\frac{\\theta}{\\pi}\\) </li> </ul> </li> </ul> Texture mapping on sphere"},{"location":"texture_mapping/#texture-mapping-on-triangle","title":"Texture mapping on triangle","text":"<ul> <li>At any point \\((\\beta, \\gamma)\\) inside a triangle, the \\((u, v)\\) coordinates can be computed as<ul> <li>\\(u = = (1 - \\beta - \\gamma)u_a + \\beta u_b + \\gamma u_c\\)</li> <li>\\(v = = (1 - \\beta - \\gamma)v_a + \\beta v_b + \\gamma v_c\\)</li> </ul> </li> <li>Here, \\((u_a, v_a)\\), \\((u_b, v_b)\\), and \\((u_c, v_c)\\) are the texture coordinates at the vertices of the triangle.</li> </ul>"},{"location":"texture_mapping/#perspective-correct-texture-mapping","title":"Perspective correct texture mapping","text":"<ul> <li>Accounts for the actual 3D position of the point on the surface.</li> <li>Screen space interpolation is used to compute \\((u, v)\\) at each pixel.</li> </ul> \\[f_x = (1 - \\alpha) \\frac{x_0} {w_0} + \\alpha \\frac{x_1} {w_1}\\] <ul> <li>Perspective correct interpolation of \\((u, v)\\) is given by</li> </ul> \\[f_x = \\frac{(1 - \\beta) x_0 + \\beta x_1} {(1 - \\beta) w_0 + \\beta w_1}\\] <ul> <li>Post Perspective, use </li> </ul> \\[\\beta = \\frac{\\alpha * w_0}{(1 - \\alpha) w_1 + \\alpha w_0}\\]"},{"location":"texture_mapping/#texture-coordinates","title":"Texture coordinates","text":"<p>How to specify mapping from an object to a texture? It should have the following properties</p> <ul> <li>Piecewise Linearity: in order to use interpolation hardware</li> <li>Invertible: go back and forth between object and texture space</li> <li>Easy to Compute</li> <li>Area Preserving or Angle Preserving</li> </ul> <p>Some Common texture mappings are </p> <ul> <li>Linear/Planar/Hyperplanar Projection</li> <li>Cylindrical </li> <li>Spherical</li> <li>Piecewise-linear or Piecewise-Bilinear on a plane from explicit per-vertex texture coordinates</li> <li>Normal-vector projection onto a sphere</li> </ul> <p>Note</p> <p>Look at slides for images</p> Texture Mapping Description Use case Planar Texturing Image is projected orthographically onto the object. Texturing flat surfaces Cylindrical Texturing Image is projected onto the object using a cylinder. Texturing cylinders, cones, and other objects that have some central axis Spherical Texturing Image is projected onto the object using a sphere. Texturing blobs, spheres, and other objects that are roughly spherical Box Texturing Image is projected onto the object from all six sides of a cube. Texturing cubes and other objects that are roughly box-shaped UV Texturing Explicit per-vertex texture coordinates are used to map the texture onto the object. Texturing objects with complex geometry"},{"location":"texture_mapping/#texture-filtering","title":"Texture filtering","text":"<ul> <li>During texture mapping, one pixel on a textured surface may not correspond to one texel.</li> <li>Insufficient resolution may lead to aliasing artifacts/jaggies.</li> <li>Can lead to both minification and magnification artifacts.</li> </ul> Method Description Nearest Neighbor Simplest method, picks the nearest texel. Bilinear Find closest matching source pixel grid and interpolate between 4 neighboring texels. Mipmapping Precompute multiple versions of the texture at different resolutions using high-quality filtering, and then do nearest neighbor or bilinear filtering on the mipmap. Anisotropic Recognize when a texture is being stretched in one direction more than another and takes larger number of samples to compensate."},{"location":"texture_mapping/#bilinear-interpolation","title":"Bilinear Interpolation","text":"<p>Say we are given a 2D image, and we want to estimate the value of a pixel at a non-integer coordinate. We can use bilinear interpolation to estimate the value of the pixel. </p> Bilinear Interpolation <p>We first start off with finding the value of \\(y_1\\) and \\(y_2\\) using linear interpolation.</p> \\[ \\begin{align*}     y_1 &amp;= a + (b - a) \\cdot t \\\\     y_2 &amp;= c + (d - c) \\cdot t \\end{align*} \\] <p>Then, we interpolate these values across the other axis to get the final value.</p> \\[y = y_1 + (y_2 - y_1) \\cdot q\\]"},{"location":"texture_mapping/#texture-synthesis","title":"Texture Synthesis","text":"<p>Procedural texture: little programs that computes color as a function of \\((x, y, z)\\).</p> <p>Types of procedural textures</p> <ul> <li>Fourier-like synthesis</li> <li>Perlin noise</li> <li>Reaction-diffusion textures - Simulating formation of patterns in nature, which evolve through diffusion and reaction.</li> <li>Data-driven textures - Use data from real-world sources to generate textures. Using minimal error boundary cut to create seamless textures.</li> </ul>"},{"location":"texture_mapping/#fourier-like-synthesis","title":"Fourier-like Synthesis","text":"<p>Displacement function \\(d(x, y)\\) is represented as a sum of sinusoids.</p> \\[d(x, y) = \\sum_{i=0}^{n-1} c_i cos(a_i x + b_i y + \\phi_i)\\] <p>Where \\((a_i, b_i)\\) are chosen randomly from annular region in plane \\(r^2 \\leq a_i^2 + b_i^2 \\leq R^2\\). This can be used for representing cloud density, placement of vegetation, etc.</p>"},{"location":"texture_mapping/#perlin-noise","title":"Perlin Noise","text":"<p>Perlin noise is a type of gradient noise developed by Ken Perlin. It is used in computer graphics for procedural texture generation. It is used to generate textures that look like natural phenomena such as clouds, fire, and marble.</p> Perlin Noise <p>The basic idea is to generate a grid of random gradient vectors, and then interpolate between them to get a smooth continuous noise function. The noise function is then used to generate textures.</p> <p>Consider a simpler example in 1D. The values at integer are zero, but they have random gradient vectors. The values at non-integer points are interpolated between the two nearest integer points.</p> Perlin Noise in 1D <p>For a point \\(p(x, y, z)\\) in the grid,</p> \\[n(x, y, z) = \\sum_{i = \\lfloor x  \\rfloor}^{\\lfloor x  \\rfloor + 1} \\sum_{j = \\lfloor y  \\rfloor}^{\\lfloor y  \\rfloor + 1} \\sum_{k = \\lfloor z  \\rfloor}^{\\lfloor z  \\rfloor + 1} \\Omega_{ijk} (x - i, y - j, z - k)\\] <p>Where </p> \\[\\Omega_{ijk}(u, v, w) = \\omega(u) \\omega(v) \\omega(w) (\\Gamma_{ijk} \\cdot (u, v, w))\\] \\[\\omega(t) = 2 |t|^3 - 3 |t|^2 + 1 , |t| &lt; 1, 0 \\text{ otherwise }\\] Function Meaning \\(\\Gamma_{ijk}\\) Random gradient vector at \\((i, j, k)\\) \\(\\omega(t)\\) Weight function, also called ease curves \\(\\Omega_{ijk}\\) Weighted gradient vector"},{"location":"texture_mapping/#algorithm","title":"Algorithm","text":"<pre><code>def PerlinNoise(p: Point):\n    n: float = 0 # noise value\n    for (Point q : neighborhood of p)\n        g = gradient(q) # precomputed random gradient\n        d = p - q\n        n += dot(d, g) * weight(d)\n</code></pre> <p>Note</p> <p>Look at slides for images</p>"},{"location":"texture_mapping/#other-discrete-techniques","title":"Other Discrete Techniques","text":""},{"location":"texture_mapping/#bump-mapping","title":"Bump Mapping","text":"<p>Note</p> <p>Look at slides for images</p> <p>Creates the illusion of surface detail by perturbing the surface normal using a height map. Let \\(p(u, v)\\) be a point on a parametric surface. The normal is given by</p> \\[N(u, v) = \\frac{p_u \\times p_v}{|p_u \\times p_v|}\\] <p>Where \\(p_u\\) and \\(p_v\\) are the partial derivatives of \\(p\\) with respect to \\(u\\) and \\(v\\).</p> <p>Now let the the displacement be given by a displacement map \\(d(u, v)\\), then the displaced surface can be written as \\(p' = p + d(u, v)N(u, v)\\), and displaced normal becomes</p> \\[N' = p_u' \\times p_v'\\] <p>Computing partial derivatives of \\(p'\\), we get</p> \\[ \\begin{align*} p_u' &amp;= p_u + \\frac{\\partial d}{\\partial u}N + d \\frac{\\partial N}{\\partial u}\\\\ p_v' &amp;= p_v + \\frac{\\partial d}{\\partial v}N + d \\frac{\\partial N}{\\partial v} \\\\ \\end{align*} \\] <p>For small displacements, we can ignore the second order terms and get</p> \\[N' \\approx N + \\frac{\\partial d}{\\partial u}N \\times p_v + \\frac{\\partial d}{\\partial v}N \\times p_u\\]"},{"location":"texture_mapping/#normal-mapping","title":"Normal Mapping","text":"<p>Instead of computing the normal at each point, we store the normal in a texture map as RBG values. There are two ways to store the normal in the texture map</p> <ul> <li>Object space normal mapping: Normal is stored in the object space of the surface. This allows the map to be applied to a specific surface.</li> <li>Tangent space normal mapping: Normal is stored in the tangent space of the surface. This allows the map to be applied to any surface. </li> </ul> <p>Why is Tangent-space map blue in color?</p> <p>The tangent space is defined by the surface normal, the tangent, and the bitangent. The normal map is stored in the RGB channels of the texture. The red, green, and blue channels correspond to the x, y, and z components of the normal in the tangent space. The normal map is blue in color because the z component of the normal is stored in the blue channel.</p> <p>To accurately compute tangent space,</p> <ul> <li>\\(m = \\frac{n}{|n|}\\), where \\(m\\) is the normal vector</li> <li>\\(t = \\frac{p_u}{|p_u|}\\), where \\(t\\) is the tangent vector</li> <li>\\(b = m \\times t\\), where \\(b\\) is the bitangent vector</li> </ul> <p>Formulate the rotation matrix \\(M\\) as</p> \\[M = \\begin{bmatrix} t &amp; b &amp; m \\end{bmatrix}^\\top\\] <p>that will convert representations in original space to tangent space.</p>"},{"location":"texture_mapping/#displacement-mapping","title":"Displacement Mapping","text":"<ul> <li>Problem with bump mapping: only affects normals, not geometry. Therefore cannot cast shadows, nor occlude objects.</li> <li>For more realistic effects, we can apply displacement mapping that actually moves the vertices of the object.</li> <li>Rest of the pipeline remains the same.</li> </ul>"},{"location":"texture_mapping/#environment-mapping","title":"Environment Mapping","text":"<p>Environment map is used for objects to have specular reflections of the surrounding environment. The environment is modelled as a box textured with a cube map</p> <p>For any reflection ray \\(r\\), return color from the environment map.</p>"},{"location":"transformations/","title":"Geometric Transformations","text":"<p>Transformations are very fundamental in computer graphics. They are used for Object Modelling, Viewing, and Projection. </p> <p>Objects in a scene are a collection on points which have some location, orientation and size. These properties can be changed by Translations(\\(\\mathbf{T}\\)), Rotations(\\(\\mathbf{R}\\)), and Scaling (\\(\\mathbf{S}\\)).</p>"},{"location":"transformations/#homogeneous-coordinates","title":"Homogeneous Coordinates","text":"<p>We can represent Rotation and Scaling as 2x2 matrices, but we cannot do so for Translation. There is non-linearity in Translation. To make Translation linear, we use Homogeneous Coordinates. This allows us to represent all 3 2D transformations as 3x3 matrices.</p> Proof that translation is non-linear <p>Let \\(\\mathbf{T}(p) = p + t\\) be the translation of point \\(p\\) by vector \\(t\\). Then,</p> \\[\\mathbf{T}(p + q) = p + q + t \\neq \\mathbf{T}(p) + \\mathbf{T}(q)\\] \\[\\mathbf{T}(ap) = ap + t \\neq a\\mathbf{T}(p)\\] <p>Hence, Translation is non-linear.</p> <p>To convert a point \\((x, y)\\) to Homogeneous Coordinates, we do the following:</p> \\[\\begin{bmatrix}x\\\\y\\end{bmatrix} \\rightarrow \\begin{bmatrix}wx\\\\wy\\\\w\\end{bmatrix}, w \\neq 0\\] <p>where \\(w\\) is a scaling factor. We can convert back to Cartesian Coordinates by dividing by \\(w\\). For linear transformations, embed the 2x2 matrix in a 3x3 matrix by adding a row and a column of 0s and a 1 in the bottom right corner.</p> \\[\\begin{bmatrix}a &amp; b\\\\c &amp; d\\end{bmatrix} \\rightarrow \\begin{bmatrix}a &amp; b &amp; 0\\\\c &amp; d &amp; 0\\\\0 &amp; 0 &amp; 1\\end{bmatrix}\\]"},{"location":"transformations/#2d-transformations-in-homogeneous-coordinates","title":"2D Transformations in Homogeneous Coordinates","text":"Transformation Matrix Translation \\(\\begin{bmatrix}1 &amp; 0 &amp; t_x\\\\0 &amp; 1 &amp; t_y\\\\0 &amp; 0 &amp; 1\\end{bmatrix}\\) Rotation \\(\\begin{bmatrix}\\cos\\theta &amp; -\\sin\\theta &amp; 0\\\\\\sin\\theta &amp; \\cos\\theta &amp; 0\\\\0 &amp; 0 &amp; 1\\end{bmatrix}\\) Scaling \\(\\begin{bmatrix}s_x &amp; 0 &amp; 0\\\\0 &amp; s_y &amp; 0\\\\0 &amp; 0 &amp; 1\\end{bmatrix}\\) \\(\\text{Shear}_x\\) (\\(\\phi\\) is angle along axis) \\(\\begin{bmatrix}1 &amp; \\tan{\\phi} &amp; 0\\\\0 &amp; 1 &amp; 0\\\\0 &amp; 0 &amp; 1\\end{bmatrix}\\) \\(\\text{Shear}_y\\) \\(\\begin{bmatrix}1 &amp; 0 &amp; 0\\\\ \\tan{\\phi} &amp; 1 &amp; 0\\\\0 &amp; 0 &amp; 1\\end{bmatrix}\\) <p>Properties of Rotation Matrix</p> <ul> <li>Rows are orthogonal to each other</li> <li>Columns are orthogonal to each other</li> <li>\\(\\mathbf{R}^\\top = \\mathbf{R}^{-1}\\)</li> <li>\\(\\det(\\mathbf{R}) = 1\\)</li> </ul> <p>Make sure these are always satisfied.</p>"},{"location":"transformations/#composition-of-transformations","title":"Composition of Transformations","text":"<p>To apply multiple transformations to a point, we multiply the matrices of the transformations. The order of multiplication is important. For example, to rotate and then translate a point, we would do \\(\\mathbf{T}\\mathbf{R}p\\). Operations are evaluated from right to left.</p> <p>Order of Multiplication</p> <p>In general, Matrix Multiplication is not commutative. \\(\\mathbf{AB} \\neq \\mathbf{BA}\\).</p>"},{"location":"transformations/#transformation-about-arbitrary-point","title":"Transformation about Arbitrary Point","text":"<p>To rotate a point about an arbitrary point \\((p_x, p_y)\\), we first translate the point to the origin, rotate it, and then translate it back.</p> \\[ \\mathbf{R}_p(\\phi) = \\mathbf{T}(p_x, p_y) \\hspace{10px} \\mathbf{R}(\\phi) \\hspace{10px} \\mathbf{T}(-p_x, -p_y) \\] <p>Similarly for scaling, we can perform the following:</p> \\[ \\mathbf{S}_p(s_x, s_y) = \\mathbf{T}(p_x, p_y) \\hspace{10px} \\mathbf{S}(s_x, s_y) \\hspace{10px} \\mathbf{T}(-p_x, -p_y) \\]"},{"location":"transformations/#decomposition-of-transformations","title":"Decomposition of Transformations","text":"<p>Any 2D matrix can be decomposed into a product of rotation, scale, rotation. </p> <ul> <li>Eigenvalue Decomposition: \\(\\mathbf{A} = \\mathbf{R} \\hspace{10px} \\mathbf{S} \\hspace{10px} \\mathbf{R}^{\\top}\\) </li> </ul> <p>Note</p> <p>Eigenvalue Decomposition only works for symmetric matrices.</p> <p>Info</p> <ul> <li>\\(\\mathbf{R}^{\\top}\\): Rotate the object to X and Y axes.</li> <li>\\(\\mathbf{S}\\): Scale the object.</li> <li>\\(\\mathbf{R}\\): Rotate the object back to its original orientation.</li> </ul> <p>In a way it's directional scaling.</p> <ul> <li> <p>SVD Decomposition: \\(\\mathbf{A} = \\mathbf{U} \\hspace{10px} \\mathbf{S} \\hspace{10px} \\mathbf{V}^{\\top}\\)</p> </li> <li> <p>Paeth Decomposition: Applies only to rotations. Uses shear to represent non-zero rotations. Very useful in raster image rotations.</p> </li> </ul> \\[ \\begin{bmatrix} \\cos\\theta &amp; -\\sin\\theta \\\\ \\sin\\theta &amp; \\cos\\theta \\end{bmatrix} = \\begin{bmatrix} 1 &amp; \\frac{\\cos \\phi - 1}{\\sin \\phi} \\\\ 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 0 \\\\ \\sin \\phi &amp; 1 \\end{bmatrix} \\begin{bmatrix} 1 &amp; \\frac{ \\cos \\phi - 1}{\\sin \\phi} \\\\ 0 &amp; 1 \\end{bmatrix} \\]"},{"location":"transformations/#3d-transformations","title":"3D Transformations","text":"<p>Properties which were applicable in 2D transformations are also applicable in 3D transformations. The only difference is that we have an additional axis.</p> Transformation Matrix Translation \\(\\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; t_x\\\\0 &amp; 1 &amp; 0 &amp; t_y\\\\0 &amp; 0 &amp; 1 &amp; t_z\\\\0 &amp; 0 &amp; 0 &amp; 1\\end{bmatrix}\\) Rotation about X-axis \\(\\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; 0\\\\0 &amp; \\cos\\theta &amp; -\\sin\\theta &amp; 0\\\\0 &amp; \\sin\\theta &amp; \\cos\\theta &amp; 0\\\\0 &amp; 0 &amp; 0 &amp; 1\\end{bmatrix}\\) Rotation about Y-axis \\(\\begin{bmatrix}\\cos\\theta &amp; 0 &amp; \\sin\\theta &amp; 0\\\\0 &amp; 1 &amp; 0 &amp; 0\\\\-\\sin\\theta &amp; 0 &amp; \\cos\\theta &amp; 0\\\\0 &amp; 0 &amp; 0 &amp; 1\\end{bmatrix}\\) Rotation about Z-axis \\(\\begin{bmatrix}\\cos\\theta &amp; -\\sin\\theta &amp; 0 &amp; 0\\\\\\sin\\theta &amp; \\cos\\theta &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; 1 &amp; 0\\\\0 &amp; 0 &amp; 0 &amp; 1\\end{bmatrix}\\) Scaling \\(\\begin{bmatrix}s_x &amp; 0 &amp; 0 &amp; 0\\\\0 &amp; s_y &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; s_z &amp; 0\\\\0 &amp; 0 &amp; 0 &amp; 1\\end{bmatrix}\\)"},{"location":"transformations/#arbitrary-axis-rotation","title":"Arbitrary Axis Rotation","text":"<p>Given 3 mutually orthogonal unit vectors \\(\\mathbf{u} = (x_u, y_u, z_u)\\), \\(\\mathbf{v} = (x_v, y_v, z_v)\\), \\(\\mathbf{w} = (x_w, y_w, z_w)\\), \\(R_{uvw}\\) transforms the coordinate system from \\(xyz\\) to \\(uvw\\).</p> \\[ R_{uvw} = \\begin{bmatrix} x_u &amp; y_u &amp; z_u \\\\ x_v &amp; y_v &amp; z_v \\\\ x_w &amp; y_w &amp; z_w\\end{bmatrix} = \\begin{bmatrix} u \\\\ v \\\\ w \\end{bmatrix} \\] <p>To rotate around arbitrary vector \\(\\mathbf{a} = (x_a, y_a, z_a)\\)</p> <ul> <li>Form an orthonormal \\(uvw\\) coordinate system with \\(w = \\mathbf{a}\\) </li> <li>Rotate \\(uvw\\) basis to canonical basis \\(xyz\\)</li> <li>Rotate about \\(z\\) axis by \\(\\phi\\)</li> <li>Rotate back to \\(uvw\\) basis</li> </ul> \\[R_a(\\phi) = R_{uvw}^\\top \\hspace{10px} R_z(\\phi) \\hspace{10px} R_{uvw}\\]"},{"location":"transformations/#constructing-a-basis-from-a-vector","title":"Constructing a basis from a vector","text":"<ul> <li>make \\(w\\) a unit vector \\(w = \\frac{a}{||a||}\\)</li> <li>Choose any vector \\(t\\) not parallel to \\(w\\), and build \\(u = \\frac{t \\times w}{||t \\times w||}\\)</li> <li>Complete the basis by \\(v = w \\times u\\)</li> </ul>"},{"location":"transformations/#transforming-normals","title":"Transforming Normals","text":"<ul> <li>Normals are very common in computer graphics, so we need a way to also transform surface normals.</li> <li>They do not transform the way surfaces do </li> <li>Let the surface points be transformed by the matrix \\(\\mathbf{M}\\). Then the normal vectors are transformed by the the following</li> </ul> \\[ \\mathbf{N'} = \\mathbf{M}^{-\\top} \\mathbf{N} \\] Proof of Transforming Normals <p>Say you have a point \\(p\\) and a normal \\(n\\) on a surface. The normal is perpendicular to the surface at that point, i.e.</p> \\[n^\\top p = 0\\] <p>If we transform the point to \\(p' = \\mathbf{M}p\\), then the new normal \\(n'\\) should be perpendicular to the new surface at \\(p'\\), i.e.</p> \\[n'^\\top p' = 0\\] <p>Let the new normal be \\(n' = \\mathbf{N}n\\). Then,</p> \\[n'^\\top p' = (\\mathbf{N}n)^\\top \\mathbf{M}p = n^\\top \\mathbf{N}^\\top \\mathbf{M}p = 0 \\] <p>Since we already know that \\(n^\\top p = 0\\), we can say that \\(\\mathbf{N}^\\top \\mathbf{M} = I\\). Hence, \\(\\mathbf{N} = \\mathbf{M}^{-\\top}\\).</p>"},{"location":"viewing/","title":"Viewing and Projections","text":""},{"location":"viewing/#types-of-projections","title":"Types of Projections","text":"<ul> <li>Orthographic/Parallel: projection lines are all parallel to each other (direction of projection or DOP)</li> <li>Perspective: projection lines converge at a single point (center of projection or COP)</li> </ul> Perspective vs Orthographic Projection <p>Note</p> <p>There are many types of projections, but all of them can be classified into these two categories.</p> <p>There are many more subtypes of Parallel projection than Perspective projection.</p>"},{"location":"viewing/#parallel-projections","title":"Parallel Projections","text":"<p>In the given figure, DOP is the direction of projection and VPN is view plane normal. Assume object face of interest lies in principal plane.</p> Types of Parallel Projections Type Description VPN \\(\\parallel\\) a principal axis? DOP \\(\\parallel\\) VPN? Multiview Orthographic Shows a single face, exact measurements Yes Yes Axonometric Adjacent faces, none exact, uniformly foreshortened No Yes Oblique adjacent faces, one exact, others uniformly foreshortened Yes No What the hell is foreshortening? <p>Foreshortening is the visual effect or optical illusion that causes an object or distance to appear shorter than it actually is because it is angled toward the viewer.</p>"},{"location":"viewing/#multiview-orthographic-projections","title":"Multiview Orthographic Projections","text":"<ul> <li>Projectors are orthogonal to the projection plane</li> <li>Projection plane is parallel to one of the principal planes</li> <li>Think of taking front view, top view, and side view of an object</li> </ul>"},{"location":"viewing/#axonometric-projections","title":"Axonometric Projections","text":"Different types of Axonometric Projections Type Description   Isometric All three principal axes equal (\\(120^\\circ\\)) Dimetric Two principal axes are equal Trimetric All three principal axes are different <p>Skipping Oblique Projections for now.</p>"},{"location":"viewing/#perspective-projections","title":"Perspective Projections","text":"<ul> <li>Describes the way we see the world</li> <li>Parallel lines don't remain parallel post projection (Foreshortening is not uniform)</li> <li>View Point is the point from which the object is viewed. It's an actual point</li> <li>Vanishing Points are points where parallel lines converge. They are imaginary points.</li> </ul> Perspective Projections Type Description One-point Perspective All parallel lines converge to a single point Two-point Perspective All parallel lines converge to one of two points Three-point Perspective All parallel lines converge to one of three points <p>There can be more than three vanishing points as well, and that would be an N-point perspective.</p>"},{"location":"viewing/#viewing-transformations","title":"Viewing Transformations","text":"<p>Mapping 3D locations in the canonical coordinate system to coordinates in the image. Composed of:</p> <ul> <li>A camera transformation or eye transformation</li> <li>A projection transformation</li> <li>A viewport transformation or windowing transformation</li> </ul> <p>Note</p> <p>Cannonical View Volume (CVV) is a cube containing all 3D points whose coordinates are in the range \\([-1, 1]\\).</p> <p>Viewing Frustum</p> <p>The viewing frustum is the region of space in the world that is visible in the image. It is the region of space that is mapped to the CVV.</p> <p>The pipeline is as follows</p> Viewing Transformation Pipeline"},{"location":"viewing/#camera-transformation","title":"Camera Transformation","text":"<p>Positioning the camera to look in a particular direction</p> <ul> <li>The eye position \\(e\\)</li> <li>The gaze direction \\(g\\)</li> <li>The view-up vector \\(t\\)</li> </ul> <p>The user specifies viewing as a tripled \\((e, g, t)\\), using which we create basis vectors \\(u, v, w\\) as follows:</p> <ul> <li>\\(w = \\frac{-g} {\\lVert g \\rVert }\\)</li> <li>\\(u = \\frac{t \\times w} {\\lVert t \\times w \\rVert }\\)</li> <li>\\(v = w \\times u\\)</li> </ul> <p>Basic idea: Convert the camera to the origin and align the camera with the \\(z\\)-axis.</p> \\[ M_{w \\rightarrow c} = \\begin{bmatrix} u &amp; v &amp; w &amp; e \\\\  0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}^{-1} = \\begin{bmatrix}     x_u &amp; x_v &amp; x_w &amp; 0 \\\\     y_u &amp; y_v &amp; y_w &amp; 0 \\\\     z_u &amp; z_v &amp; z_w &amp; 0 \\\\     0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix}  \\begin{bmatrix}     1 &amp; 0 &amp; 0 &amp; -x_e \\\\     0 &amp; 1 &amp; 0 &amp; -y_e \\\\     0 &amp; 0 &amp; 1 &amp; -z_e \\\\     0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] \\[ M_{c \\rightarrow w} = \\begin{bmatrix} u &amp; v &amp; w &amp; e \\\\  0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] Why is \\(w = -g\\)? <p>If we change the direction of \\(w\\), the direction of \\(u\\) changes, and it looks like we have flipped the camera.</p> <p> How sign of \\(w\\) affects \\(u, v\\) </p>"},{"location":"viewing/#projection-transformation","title":"Projection Transformation","text":"<p>Project points from camera space to the canonical view volume.</p>"},{"location":"viewing/#orthographic-projection","title":"Orthographic Projection","text":"<p>View Volume is a rectangular parallelepiped \\([l, r] \\times [b, t] \\times [n, f]\\). The steps are:</p> <ul> <li>Move the center of the view volume to the origin</li> <li>Scale the view volume to the canonical view volume</li> </ul> \\[ M_{ortho_{t}} = \\begin{bmatrix}     1 &amp; 0 &amp; 0 &amp; -\\frac{l+r}{2} \\\\     0 &amp; 1 &amp; 0 &amp; -\\frac{t+b}{2} \\\\     0 &amp; 0 &amp; 1 &amp; -\\frac{n+f}{2} \\\\ \\end{bmatrix} , \\hspace{20px} M_{ortho_{s}} = \\begin{bmatrix}     \\frac{2}{r-l} &amp; 0 &amp; 0 &amp; 0 \\\\     0 &amp; \\frac{2}{t-b} &amp; 0 &amp; 0 \\\\     0 &amp; 0 &amp; \\frac{2}{f-n} &amp; 0 \\\\     0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] \\[ M_{orth} = M_{ortho_s} \\times M_{ortho_t} =  \\begin{bmatrix}     \\frac{2}{r-l} &amp; 0 &amp; 0 &amp; -\\frac{r+l}{r-l} \\\\     0 &amp; \\frac{2}{t-b} &amp; 0 &amp; -\\frac{t+b}{t-b} \\\\     0 &amp; 0 &amp; \\frac{2}{f-n} &amp; -\\frac{f+n}{f-n} \\\\     0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\]"},{"location":"viewing/#perspective-projection","title":"Perspective Projection","text":"<p>View Volume is a frustum with the apex at the origin. Perspective projection is not affine, so we need to use homogeneous coordinates.</p> <ul> <li>\\(d\\) is the distance from the eye to the image plane</li> <li>\\(x_s = \\frac{d}{z} x\\)</li> <li>\\(y_s = \\frac{d}{z} y\\)</li> </ul> <p>The perspective projection matrix, for a frustum with \\(n\\) and \\(f\\) as near and far planes, is:</p> \\[ M_{persp} = \\begin{bmatrix}     n &amp; 0 &amp; 0 &amp; 0 \\\\     0 &amp; n &amp; 0 &amp; 0 \\\\     0 &amp; 0 &amp; n+f &amp; -fn \\\\     0 &amp; 0 &amp; 1 &amp; 0 \\end{bmatrix} \\] \\[ \\begin{pmatrix}     x \\\\ y \\\\ z \\\\ 1 \\end{pmatrix}  \\rightarrow   \\begin{pmatrix}     nx \\\\ ny \\\\ (n + f)z - fn \\\\ z  \\end{pmatrix} \\rightarrow \\begin{pmatrix}     \\frac{n}{z} x \\\\ \\frac{n}{z} y \\\\ (n + f - \\frac{fn}{z}) \\\\ 1 \\end{pmatrix} \\] <p>What is going on in the 3rd row and 4th row?</p> <ul> <li>We use one in the 4th row to introduce \\(w = z\\), which we would divide by later. to get \\(\\frac{d}{z} x\\) and \\(\\frac{d}{z} y\\). </li> <li>The 3rd row is to maintain the \\(z\\) value. We need to maintain the \\(z\\) value to calculate the depth of the object. We only care about the relative ordering so, we get </li> </ul> \\[\\tilde{z}(t) = n + f - \\frac{nf}{t}\\] <p>Where \\(t\\) is the \\(z\\) value of the object in the camera space. Notice that \\(\\tilde{z}(n) = n\\) and \\(\\tilde{z}(f) = f\\).</p>"},{"location":"viewing/#full-perspective-transformation","title":"Full Perspective Transformation","text":"\\[ M_{persp} = M_{orth} \\times M_{persp} \\] \\[ M_{persp} = \\begin{bmatrix}     \\frac{2n}{r-l} &amp; 0 &amp; \\frac{r+l}{r-l} &amp; 0 \\\\     0 &amp; \\frac{2n}{t-b} &amp; \\frac{t+b}{t-b} &amp; 0 \\\\     0 &amp; 0 &amp; \\frac{f+n}{f-n} &amp; -\\frac{2fn}{f-n} \\\\     0 &amp; 0 &amp; 1 &amp; 0 \\end{bmatrix} \\]"},{"location":"viewing/#viewport-transformation","title":"Viewport Transformation","text":"<p>Map the canonical view volume to the screen coordinates. Image boundaries have a half-unit offset. We want to map \\([-1, 1]^2\\) to \\([-0.5, n_x - 0.5] \\times [-0.5, n_y - 0.5]\\)</p> <p>The transformation to map points in \\([x_l, x_h] \\times [y_l, y_h]\\) to \\([x_l', x_h'] \\times [y_l', y_h']\\) is called Windowing Transformation. The steps are:</p> <ul> <li>Move \\([x_l, y_l]\\) to \\([0, 0]\\)</li> <li>Scale the rectangle to the target rectangle</li> <li>Move the rectangle to \\([x_l', y_l']\\)</li> </ul> \\[ M_{win} = \\begin{bmatrix}     1 &amp; 0 &amp; x_l' \\\\     0 &amp; 1 &amp; y_l' \\\\     0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix}     \\frac{x_h' - x_l'}{x_h - x_l} &amp; 0 &amp; 0 \\\\     0 &amp; \\frac{y_h' - y_l'}{y_h - y_l} &amp; 0 \\\\     0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix}     1 &amp; 0 &amp; -x_l \\\\     0 &amp; 1 &amp; -y_l \\\\     0 &amp; 0 &amp; 1 \\end{bmatrix} \\] <p>For the screen coordinates, we have \\([0, n_x] \\times [0, n_y]\\), so \\(x_l = 0, x_h = n_x, y_l = 0, y_h = n_y\\).</p> \\[ M_{vp} = \\begin{bmatrix}     \\frac{n_x}{2} &amp; 0 &amp; 0 &amp; \\frac{n_x - 1}{2} \\\\     0 &amp; \\frac{n_y}{2} &amp; 0 &amp; \\frac{n_y - 1}{2} \\\\     0 &amp; 0 &amp; 1 &amp; 0 \\\\     0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] <p>Note</p> <p>The \\(z\\)-coordinate is preserved after the viewport transformation, as it will be used for depth testing.</p>"},{"location":"visibility_determination/","title":"Visibility Determination","text":"<p>Determining the surface patches that will be be visible from a given viewpoint, also called hidden surface removal. There are mainly 3 techniques to do this:</p> <ol> <li>Object precision</li> <li>Image precision</li> <li>List priority(hybrid object/image precision)</li> </ol>"},{"location":"visibility_determination/#comparison-of-object-and-image-precision","title":"Comparison of Object and Image Precision","text":"<p>Object precision algorithms take the basic structure as following</p> <pre><code>for each object O:\n    find the part A of object that is visible\n    display A \n</code></pre> <p>Image precision algorithms take the basic structure as following</p> <pre><code>for each pixel P on the screen:\n    Let R be the ray from viewpoint through P\n    determine the visible object O pierced by R\n    if there is such O:\n        display the pixel in the color of O\n    else:\n        display the pixel in the background color\n</code></pre> Object Precision Image Precision Computes all visible parts Determines visible parts for each pixel Complexity based on number of objects Complexity based on number of pixels"},{"location":"visibility_determination/#back-face-culling-object-precision","title":"Back Face Culling (Object Precision)","text":"<ul> <li>Discard back facing polygons from rendering</li> <li>Consider counter-clockwise orientation outward. Discard face if \\(\\hat{v} \\cdot \\hat{n} &lt; 0\\)</li> </ul> Back face culling"},{"location":"visibility_determination/#ray-casting-image-precision","title":"Ray Casting (Image Precision)","text":"<p>Computes the visibility function. \\(R\\) is a ray with origin \\(Q\\) and direction \\((P' - Q)\\), where \\(P'\\) is the pixel center. The visibility function is defined as</p> \\[ V(P, Q) = \\begin{cases} 1 &amp; \\text{if } P \\text{ is the result of intersection of query on ray} R \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\] <ul> <li>Partition the project plane into pixels</li> <li>For each pixel, construct a ray emanating from the eye/camera passing through the center of the pixel and into the scene</li> <li>Intersect the ray with every object in the scene</li> <li>Store the first hit object and its color</li> </ul>"},{"location":"visibility_determination/#warnocks-algorithm-image-precision","title":"Warnock's Algorithm (Image precision)","text":"<ul> <li>Elegant divide-and-conquer hidden surface algorithm</li> <li>Relies on area coherence of polygons to resolve visibility of many polygons in image space</li> <li>Each polygon has one of the four relationships to the area of interest </li> </ul> Warnock's Algorithm Base Cases      <pre><code>def warnock(region):\n    if all(polygons are disjoint):\n        fill(background color)\n    else if only one intersecting or contained polygon:\n        fill(background color)\n        scan convert the polygon\n    else if any(surrounding polygon):\n        fill(polygons color)\n    else:\n        divide the area into 4 quadrants\n        for each quadrant:\n            warnock(quadrant)\n</code></pre> Warnock's Quad Tree      <p>Subdivision continues till the resolution of image is reached, and then polygons are sorted by z-values and the closest polygon is displayed.</p>"},{"location":"visibility_determination/#z-buffer-algorithm-image-precision","title":"Z-Buffer Algorithm (Image Precision)","text":"<ul> <li>Record depth information for each pixel</li> <li>Z-buffer is a 2D array of same size as the frame-buffer, which stores depth as real values</li> <li>Scan convert primitives in frame-buffer and Z-buffer</li> </ul> <pre><code>Initialize FRAME_BUFFER to background color\nInitialize DEPTH ti infinite\nfor each face:\n    for each point p of F:\n        if projects to FRAME_BUFFER[i, j]:\n            if depth(p) &lt; DEPTH[i, j]:\n                FRAME_BUFFER[i, j] = color(p)\n                DEPTH[i, j] = depth(p)\n</code></pre>"},{"location":"visibility_determination/#z-buffer-precision-problems","title":"Z-Buffer Precision Problems","text":"<ul> <li>In practice, the z-values in the buffer are non-negative integers as it's faster to process integers over true floats</li> <li>Using an integer range of \\(B\\) values \\(\\{0, 1, 2, \\ldots, B-1\\}\\)<ul> <li>Map 0 to near plane and \\(B-1\\) to far plane</li> <li>\\(z, n, f &gt; 0\\) w.r.t. the camera space</li> </ul> </li> <li>Each z-value is sent to a bucket with depth</li> </ul> \\[\\Delta z = \\frac{f - n}{B}\\] <p>If \\(b\\) bits are used to store the z-value, then \\(B = 2^b\\). We need enough bits to make sure any triangle in front of another triangle will have it's depth mapped to distinct depth bins. For example, in a scene where triangles have a separation of at least 1 meter, \\(\\Delta z &gt; 1\\) will be sufficient.</p> <p>Two ways to make \\(\\Delta z\\) smaller:</p> <ul> <li>Move \\(n, f\\) closer to each other</li> <li>Increase the number of bits \\(b\\), but this is often not possible due to hardware limitations</li> </ul> <p>In case of perspective transformation, </p> \\[z = n + f - \\frac{f \\cdot n}{z_w}\\] <p>where \\(z_w\\) is the depth in world space, and \\(z\\) is post-perspective transformation depth. Now, bin size vary with depth </p> \\[\\Delta z_w \\approx \\frac{z_w^2 \\Delta z}{fn}\\] <p>Largest bin size is for \\(z_w = f\\), and we cannot choose \\(n = 0\\). To make \\(\\Delta z_w^{\\text{max}}\\) as small as possible, we need to minimize \\(f\\) and maximize \\(n\\). This is why it's important to choose \\(n, f\\) as close as possible to each other.</p> <p>If \\(\\Delta z_w^{\\text{max}}\\) is too large, then we see Z-fighting artifacts</p> Z-fighting"},{"location":"visibility_determination/#painters-algorithm-bsp-tree-algorithm-list-priority","title":"Painter's Algorithm - BSP Tree Algorithm (List Priority)","text":"<ul> <li>Faces in the scene are sorted by their distance from the camera, and drawn in that order</li> <li>Cannot always be used, for example incase of piercing polygons or cyclic overlaps</li> </ul> <p>BSP(Binary Space Partitioning) Tree is a painter's algorithm with an added restriction that no polygon crosses the plane defined by any other polygon</p>"},{"location":"visibility_determination/#basic-idea","title":"Basic Idea","text":"<ul> <li>Consider two triangles \\(T_1\\) and \\(T_2\\)</li> <li>\\(T_1:f_1(p) = 0\\) \\(\\forall\\) \\(p \\in T_1\\) and let \\(f_1(p) &lt; 0\\) \\(\\forall\\) \\(p \\in T_2\\)</li> </ul> <p>Then for any viewpoint \\(e\\), correct rendering is</p> <pre><code>if f_1(e) &lt; 0:\n    draw T_1\n    draw T_2\nelse:\n    draw T_2\n    draw T_1\n</code></pre> <p>Note</p> <p>To check which side of the triangle the point is on, you need to only check the corners</p>"},{"location":"visibility_determination/#tree-construction","title":"Tree Construction","text":"<p>This observation can be generalized to many objects provided none of them span the plane defined by \\(T_1\\)</p> <p>We can construct a binary tree with </p> <ul> <li>root: \\(T_1\\)</li> <li>negative branch: objects with vertices satisfying \\(f_1(p) &lt; 0\\)</li> <li>positive branch: objects with vertices satisfying \\(f_1(p) &gt; 0\\)</li> </ul> <p><pre><code>def draw(bsptree tree, point e)\n    if tree.empty\n        return\n    if f_tree_root(e) &lt; 0\n        draw(tree.plus, e)\n        rasterize(tree.triangle)\n        draw(tree.minus, e)\n    else\n        draw(tree.minus, e)\n        rasterize(tree.triangle)\n        draw(tree.plus, e)\n</code></pre> Once the tree is pre-computed, rendering will work for any viewpoint</p> <p>What if the polygon spans the plane?</p> <ul> <li>Split the polygon into two parts</li> <li>Add the two parts to the tree</li> <li>Continue the process recursively</li> </ul> <p> BSP Tree Splitting      </p>"}]}